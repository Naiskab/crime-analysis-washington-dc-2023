---
title: "Exploratory Data Analysis on the Washigton crime report "
author: "Vishal Fulsundar, Suraj kapare, Halima Al Balushi, Naiska Buyandalai"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r dropdown="true", echo=TRUE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(warning = F, results = "markup", message = F)
# knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 

``` 

# 1)Introduction---- 

Introduction:
In our project, we aim to investigate crime patterns in Washington, DC, utilizing data from the Open Data DC portal. The dataset, "Crime Incidents in 2023," comprises 34,215 observations detailing reported crimes, including information on the type of offense, time of occurrence, geographic location, and methods involved.
As individuals, especially students, consider buying a house, renting an apartment, relocating, or traveling, one of the first questions that arises is, "Is it safe?" This concern for safety drives many to seek out neighborhoods with low crime rates before evaluating other factors such as price, amenities, and accessibility. Therefore, understanding crime dynamics is crucial not only for law enforcement but also for residents and potential newcomers to the area.
The primary focus of our analysis is to explore crime density across different areas of Washington, DC, identifying neighborhoods that experience the highest levels of criminal activity. This exploration aims to uncover patterns that can inform law enforcement resource allocation and guide community safety initiatives aimed at crime prevention. By providing insights into crime trends, we hope to empower individuals and families to make informed decisions regarding their living environments.


# 2)Data Sources:

Crime Incidents in Washington, DC (2023):
Link: Crime Incidents in 2023
Description: This dataset serves as the primary source for our analysis, encompassing 34,215 reported crime incidents in Washington, DC. It includes critical information such as the type of offense, time of occurrence, geographic location, and methods involved. This comprehensive dataset is essential for examining crime patterns and densities across different neighborhoods
 
 Our Main data frame named- Crime_data have records around 34215observation including the 25 variables. 
 The data set(crime_data) give overiew  of the potensial varibles in the dataset.These varible give /provide the details view of the each incident,enabling us to analyze across  dimensions like geographical location, type of offense, time of occurrence, and associated methods as well as timing of the occurence betwnn the shift(day,evening,midnight).
 ex 1)  Type of offence-  nature of the crime, such as theft, assault, or vandalism, allowing us to categorize and compare crime types
 2) Shift of occurence- shift the crime happened, which can reveal temporal patterns or peak crime times.
 3)Method Involved: Provides information on any specific method or weapon(knief,gun,other) used in the offense 
 4)District or Area: The specific district or police jurisdiction involved,

```{r dropdown="true", echo=TRUE}
 
crime_data <- read.csv("C:/Users/DELL/Documents/GitHub/crime-analysis-washington-dc-2023/Crime_Incidents_in_2023.csv")
head(crime_data)
str(crime_data)
```

Neighborhood Clusters:
Link: Neighborhood Clusters
Description: This dataset provides a mapping of numerical identifiers for neighborhoods to more recognizable names, such as Columbia Heights, Georgetown, and Dupont Circle. By integrating this data, we enhance the interpretability of our analysis, making it easier for stakeholders to understand the geographic context of crime data.

Cluster   has 46 observations and 2 variables and  data frame  represent neighborhood clusters in Washington, DC. 
NBH_NAMES: This column contains the full names of the neighborhoods within each cluster.
NAME: This column contains the cluster identifier (e.g., "Cluster 16", "Cluster 41"), which is a unique numerical label for each neighborhood cluster. 

```{r dropdown="true", echo=TRUE}
library(dplyr)
cluster <- read.csv("C:/Users/DELL/Documents/GitHub/crime-analysis-washington-dc-2023/Neighborhood_Clusters.csv")

cluster <- cluster %>%  select(NAME,NBH_NAMES)
head(cluster)
str(cluster)
```



DC Police Stations:
Link: DC Police Stations
Description: This dataset contains the locations of police stations across Washington, DC. Incorporating this information allows us to analyze the proximity of law enforcement resources to neighborhoods with varying crime rates, offering insights into potential coverage gaps.

Dc police station data frame have  dataframe has 15 observations and 23 variables,
some variables for examples are Zip code , address, lattitude, longitude ,Type, name. 


```{r}
# Reading Police Stations data
police_stations <- read.csv("C:/Users/DELL/Documents/GitHub/crime-analysis-washington-dc-2023/Police_Stations.csv")

head(police_stations)
str(police_stations)

```

# 2.1) Library----
WE have loaded the required library here 
 ggplot2: Used for creating static visualizations, such as line graphs, bar charts, and scatter     plots.
  
dplyr: A data manipulation package that provides functions to filter, arrange, and summarize         data.
  
lubridate: Simplifies date and time manipulation, making it easier to work with date formats,     extract time components, and perform calculations.

sf: Provides support for handling spatial data, allowing you to analyze and visualize geographic data.

leaflet: Used for creating interactive maps, often integrating spatial data from the sf package.

```{r}
# 2) load the require library 

library(ggplot2)
library(dplyr)
# ggplot library for graphs 
library(lubridate) # fro thr data manupulation 
library(sf)
library(leaflet) 
```
# 2.2) data collection--
from the is.null operator , we got that there is no record in the OCTO_RECORD_ID column. total 34215 records are missing from the records.

DISTRICT: 285 records are missing district details.
PSA (Police Service Area): 298 records lack PSA information, which might impact analyses that involve crime distribution across service areas.
CENSUS_TRACT: 16 records are missing census tract data, which could slightly affect demographic or geographic analyses at the census level.
OCTO_RECORD_ID: All 34,215 records are missing from the recoreds . 
so, we have to remove all these recoreds for the eda. From the column named OCTO_RECORD_ID al 34,215 recoreds are missing, so we have to remove this column. We do not require that column. 
del.
 

```{r}
colnames(crime_data)

colSums(is.na(crime_data))

# Check for duplicate rows from the data set 
duplicates <- crime_data[duplicated(crime_data), ]
nrow(duplicates)

head(crime_data)

```

Now, we going to  check the indual column null values, 
PSA (Police Service Area): A total of 33,917 records have non-missing values, indicating that a significant number of incidents have an associated PSA.
CENSUS_TRACT: 34,199 records are populated, suggesting that the majority of entries include census tract information.
DISTRICT: 33,930 records are complete, showing that most incidents are assigned to a specific district.
WARD: There are 5 missing records in this column, which means that only a small fraction of incidents lack associated ward information.
Overall, the dataset is mostly complete for key geographic identifiers, with the exception of the WARD column, which has a minimal number of missing values. This completeness enhances the reliability of our analysis related to crime patterns in Washington, DC.

```{r}
sum(!is.na(crime_data$PSA))          # Check rows with PSA not missing
sum(!is.na(crime_data$CENSUS_TRACT)) # Check rows with CENSUS_TRACT not missing
sum(!is.na(crime_data$DISTRICT))     # Check rows with DISTRICT not missing
sum(is.na(crime_data$WARD))          # Check rows with WARD missing

```
## Data Preprocessing---


To prepare the data, we performed a series of preprocessing steps including cleaning, handling missing values, and transforming data types

 we have to delete these null values , we had founded null from values ward , (census_tract) (geographical region ) , district,  psa (public safty areas )
To ensure the quality of our dataset, we need to remove the records with null values in the WARD, CENSUS_TRACT, DISTRICT, and PSA.

```{r}
crime_data1 <- crime_data[
    !is.na(crime_data$PSA) & 
    !is.na(crime_data$CENSUS_TRACT) & 
    !is.na(crime_data$DISTRICT) 
    , 
]
nrow(crime_data1)
# now 
```
# check Null Values-

we are geeting the result numeric-0 so that means we don't have a null value in the data set. 

cheking the duplicate values in the dataset. 
```{r}

# check duplicate values 
# Ensure crime_data1 is a data frame
crime_data1 <- as.data.frame(crime_data1)

duplicate_crime_data <- crime_data1[duplicated(crime_data1), ]
nrow(duplicate_crime_data)

head(crime_data1)
```
# 2.3)Duplicate---
 so we don't have duplicate vlaues in the data set also . 
 
now our data set is net and clean . 
we are ready for the data transformation .
now we have to convert some rows of data in to some other formate beacuse of our need .
 we some time trand=sform the solumns as according to our use of that data .
 for more readiblity of the data set , means we have the data formate YY/mm/DD HH/MM/SS, as we want to annalyis the data as month as well as day wise , we have to extract the months and days from this formate. 
 create the new columns Year and month from the column Report_dat. 
 as you can see the 2 columns are newly added at the ends of the data-frame. 

```{r}

head(crime_data1)
crime_data1$REPORT_DAT <- as.Date(crime_data1$REPORT_DAT, formate="%m/%d/%Y")

# extract the year and month also for future plots

crime_data1$Year <- format(crime_data1$REPORT_DAT,"%Y")
crime_data1$Month <- format(crime_data1$REPORT_DAT,"%m")

head(crime_data1)

```
# Reframe Data frame
we can see that , 2 new columns are added at the end of the all columns. now readjust columns, add these columns in the data frame after the REPOST_DAT. 

rearrange the columns and move the columns YEAR and Month after the report year .
 
```{r}
crime_data1 <- crime_data1 %>%
    select(1:which(colnames(crime_data1) == "REPORT_DAT"), Year, Month, everything())

head(crime_data1)
str(crime_data1)
```
#Normalization(Optional):
first thing, for the eda part stastical test, we don;t require normalization prcosess. If we want to make a model on the dataset then pnly we have to do normalzition.  
 
now we have to normalize the data set 
Beacuse ----------
1) uniformality across the featurre- in the data set we have geo location data , date time data ,character data (shift ), numerical data(disctrict ) some pincode , so our data have SACLE  are drastically different , if we don;t normalize , some time anlalysis  will affect 
 
2)  it will help in the improving  the speed in analysis 
3) avoid the feature domination effect some feature are x, y co ordinate values are greater than the ward number , so to avioid these domination we have to noramzalise the data set . 
 
 
```{r}
# select the columns , on whcih we have to process the normalize the data. 
# libriry "dplyr" require for the normalize the data . 
# Select numerical columns that need to be normalized
# Exclude categorical columns like OFFENSE, SHIFT, METHOD, etc.
# Select only numeric columns that should be normalized
numeric_columns <- crime_data1 %>%
  select(X, Y, XBLOCK, YBLOCK, LATITUDE, LONGITUDE) 
# Apply Min-Max normalization for each numeric column
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

# Apply normalization to the numeric columns
normalized_numeric_columns <- as.data.frame(lapply(numeric_columns, normalize))

# Add back the non-numeric columns to the normalized dataset
crime_data2 <- bind_cols(crime_data1 %>% select(-X, -Y, -XBLOCK, -YBLOCK, -LATITUDE, -LONGITUDE), 
                          normalized_numeric_columns)

str(crime_data2)

# view the first 5 rows of the data set 
head(crime_data2)
```
# 3) Summary Stastics--
we examined the structure of the normalized dataset and viewed the first five rows to ensure everything was in order. This normalization step will significantly enhance the quality and reliability of our analysis moving forward.

Now start the EDA- Exploratory Data Analysis
 
summary stastics-----
In this section, we present the summary statistics of the dataset, which provide an overview of the key characteristics of the data. 

```{r}
crime_data2 <- crime_data2 %>% 
  left_join(cluster, by = c("NEIGHBORHOOD_CLUSTER" = "NAME"))
summary(crime_data2)

``` 
# 3.1)key observation. 
1) report date- show the range of the date, when 1st offence dtae started to the last date of the offence date. even report date  information is avaible in the quater wise.

2) most of the varibles are catogica like ward, shift, block, methods. 
 in the next, part we will convert some of the variable into int value. 

now we will check with the count(summanry) wise variable. 

 summary provide the  stastics information for each varible in the dataset. '
 as we can see that, most of the offence happen in the ward 
 total wards - 8 , minimum offence in ward- 2 , maximum offence in the ward 5 with 5569. 
Variability: The number of offenses varies significantly across the wards, with Ward 5 having the highest total offenses and Ward 3 the lowest.
Potential Focus Areas: Wards with higher offense totals may require more resources or strategic interventions to address crime effectively.


```{r}

# Summarize total offenses by ward
offense_total_by_ward <- crime_data2 %>%
  group_by(WARD) %>%
  summarise(Total_Offenses = n())

# View the summarized data
print(offense_total_by_ward)

```
# 3.2)Ward wise total offence-- 
 first we have to know , what type of theft it is normally happening in DC. 
 for refernce
1) Theft F/Auto (Theft from Auto)---
    #This type of theft refers to property stolen from a vehicle. It typically includes items such         as: Personal belongings (e.g., bags, electronics, and clothing)
 2) Motor vehicle theft refers to the unlawful taking or stealing of a vehicle itself. This includes:Cars, trucks, motorcycles, and other motor vehicles

 3) Theft/Other--This category encompasses various types of thefts that do not fit into the more specific categories. It may include:-- item lifting from retail stores, Theft of items from homes
 4) Arson---- is the criminal act of deliberately setting fire to property. It can involve burning buildings, vehicles, or other structures
```{r}
offence_count <- crime_data2 %>%
  group_by(OFFENSE)%>%
  summarise(count=n())

print(offence_count)
```
# 3.3) Offence_type Vs count--
The numbers indicate that Theft from Auto and Motor Vehicle Theft are significant issues, highlighting the importance of vehicle security. and least with the Arson type of the offence. 

 now we are looking the offence by the shift wise with the count wise. mean we will know in which shift most of the offence happen, by which method as well as , which type of the offence. so that we can micro - analyze these things.

EVENING (13148)--- count here is the highest among the three shifts, indicating that more offenses happen during this time. Understanding the distribution of crimes by shift can help in analyzing crime trends and identifying times when specific types of crimes are more prevalent, allowing for targeted interventions
```{r}
# Group by SHIFT and summarize the count of offenses
offence_by_shift <- crime_data2 %>%
  group_by(SHIFT) %>%
  summarise(Count = n(), .groups = 'drop')
# Check the structure of the resulting table
#str(offence_by_shift)
# Print the summary table
print(offence_by_shift)
```

# 3.4)crime count by shift 

count by method --
Least Common Offenses---  The offenses with the lowest counts include various types of arson and sex abuse, predominantly occurring during the midnight and day shifts. Notably, arson incidents are quite rare, especially during evening and midnight hours.
Highest Common Offenses: Conversely, theft-related offenses dominate the statistics, particularly the "THEFT/OTHER" category, with significant occurrences in both the evening and daytime shifts. This indicates that theft crimes are more prevalent, especially during peak hours when more people are likely to be out.

```{r}
# Check if SHIFT is a factor or convert it to one if needed
crime_data2$SHIFT <- as.factor(crime_data2$SHIFT)

library(dplyr)

# Ensure SHIFT is a factor
crime_data2$SHIFT <- as.factor(crime_data2$SHIFT)

offence_by_shift <- crime_data2 %>%
  group_by(SHIFT, OFFENSE) %>%
  summarise(Count = n(), .groups = 'drop') %>%
  arrange(desc(Count))

# Check the structure of the resulting data frame
#str(offence_by_shift)
#head(offence_by_shift)
# Print the summary table
print(offence_by_shift)

```
# 3.4) District Vs Count----
District 3 has the highest total count of offenses with 6612 incidents, indicating it may have higher crime activity or reporting in comparison to others.
District 7 shows the lowest total count at 2883
```{r}
# Summarize the count of offenses by district
OFFENCE_BY_DISTRICT <- crime_data2 %>%
  group_by(DISTRICT) %>%
  summarise(count = n(), .groups = 'drop')  # 'drop' will ungroup after summarizing
# Print the summarized table
print(OFFENCE_BY_DISTRICT)
```
As we know most of the varibles are catagorical - analyze the correlation between categorical columns in  dataset, we can use various methods since correlation typically applies to numerical data.
But the numerical data we have like longitude and lattitude , as well as CNN, Report date , but we don;t require numerical data coraltion of this varible. 

# 4)EDA-- 

# 4.1)crime count by shift....
exploratory data analysis. 

This visualization helps us understand which shift have been most affected by theft incidents, providing insights into ward hotspots.

for eda part we are going to use the library ggplot,dyplr,tidyr,ggplot2.
as we can see, the shift VS shift . evening have slighty high count of offence than day. 


``` {r}

# Crime count based on Shift
library(ggplot2)

offense_by_shift_funnel <- crime_data2 %>%
  group_by(SHIFT) %>%
  summarise(Total_Offenses = n(), .groups = 'drop') %>%
  arrange(desc(Total_Offenses))  # Arrange shifts by number of offenses

# Create funnel chart
ggplot(offense_by_shift_funnel, aes(x = SHIFT, y = Total_Offenses, fill = SHIFT)) +
  geom_col(width = 0.7) +  # Bar chart with narrow bars
  coord_flip() +  # Flip to make it look like a funnel
  geom_text(aes(label = Total_Offenses), hjust = -0.2) +  # Add labels
  scale_y_reverse() +  # Reverse the y-axis to give the funnel effect
  labs(title = "Funnel Chart: Offense Counts by Shift", x = "Shift", y = "Total Offenses") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set2")

```




# 4.2) Distrubution of crime offense

distrubution of the crime offence by  type of the offence 
 
horizontal - bar plot for the offec frequency.
cord flip for more readibility of the plot , plot is not good readable as verticle so flip to horizontal.
  
```{r}
ggplot(crime_data2, aes(x = OFFENSE)) +
  geom_bar(fill = "red") +
  labs(title = "Distribution of Crime Offenses", x = "Count", y = "Offense Type") +
  theme(axis.text.y = element_text(size = 100)) +  
  theme_minimal()  +
 coord_flip()  

``` 

so the most common offence is Theft/other , followed by theft/auto --- 

most offense type in the Washigton D.C is theft , the 1st  other and on 2nd is vhehicle 

# 4.3) Distribution of different offenses by shift----
 
```{r}

# Plot the 
ggplot(crime_data2, aes(x = SHIFT, fill = OFFENSE)) +
  geom_bar(position = "dodge") +  # Use dodge to separate bars for different offenses
  labs(title = "Distribution of Offenses by Shift", x = "Shift", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, size = 10, hjust = 1)) +  # Rotate x-axis labels
  theme_minimal()

```
# 4.4)METHOD OF OFFENCE VS OFFENCE-------

Robbery Offenses: We see a noticeable spike in robbery incidents during midnight, which is pretty significant. In fact, robbery offenses outnumber motor vehicle thefts during this time!

Dangerous Weapons: There’s also a rise in the use of dangerous weapons, particularly in the evening and midnight hours. 

```{r}


ggplot(crime_data2, aes(x = METHOD, fill = OFFENSE)) +
  geom_bar(position = "dodge") +  # Use dodge to separate bars for different offenses
  labs(title = "Distribution of Offenses by Method", x = "Method", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, size = 10, hjust = 1)) +  
  theme_minimal()



```
# 4.5)Distribution of  offenses method  by shift---
Dangerous Weapons-- Gun and knif : There’s also a rise in the use of dangerous weapons, particularly in the evening and midnight hours
 
```{r}


ggplot(crime_data2, aes(x = SHIFT, fill = METHOD)) +
  geom_bar(position = "dodge") +  # Use dodge to separate bars for different methods
  labs(title = "Distribution of Offense Methods by Shift", x = "Shift", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, size = 10, hjust = 1)) +  # Rotate x-axis labels
  theme_minimal()



```
# 4.6) Distrubution of offences bt ward-
As we can see, the most common offence in every ward with the highest pick is theft/auto and theft/oter. people take aways most of the things from your car, is the most common offence. 
But as we can see,In the ward-3 every offence is at its least. 
Highest variabily if the offence is in the ward 1 and 2nd but in ward-7 all offence are nearly in same propostion. 
 
```{r}
# Convert WARD to integer

# Check unique values in the WARD column


crime_data2$WARD <- as.factor(crime_data2$WARD)


ggplot(crime_data2, aes(x = WARD, fill = OFFENSE)) +
  geom_bar(position = "dodge") +  
  labs(title = "Distribution of Offenses by Ward", x = "Ward", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, size = 10, hjust = 10)) +theme_minimal()

```
 
In above graph we can see that  - ward 5  have more offence followed by 2
 
 in the below we want to see crime by month-date s0
  Most of the crime happen in the DC of Summer season. 
 When we moving from the summer to winter season we see sudden drop of offence( may be due to harsh environment-). But when we move forward to the fall some theft increases.
 monthly Crime trend- 
 
 Now as we can see from the 2nd month onward up to the 7th month. There is positive trend happend across the offence. And sfter july there is sudden drop of the offence till december expect 10th month , there is unexpected increasing graph for the oct month. 
#

# 4.7) Seasonal Crime pattern 

```{r}

# Ensure START_DATE is in Date format
crime_data2$START_DATE <- as.Date(crime_data2$START_DATE)

# Extract year and month
crime_data2$Year <- format(crime_data2$START_DATE, "%Y")  # Extracting the year as a string
crime_data2$Month <- format(crime_data2$START_DATE, "%b")  # Extracting abbreviated month names

# Define seasons based on months (northern hemisphere)
crime_data2$Season <- case_when(
  crime_data2$Month %in% c("Dec", "Jan", "Feb") ~ "Winter",
  crime_data2$Month %in% c("Mar", "Apr", "May") ~ "Spring",
  crime_data2$Month %in% c("Jun", "Jul", "Aug") ~ "Summer",
  crime_data2$Month %in% c("Sep", "Oct", "Nov") ~ "Fall",
  TRUE ~ "Unknown"  # Catch-all for any unexpected month values
)

# Group by season and summarize total crime count per season
crime_by_season <- crime_data2 %>%
  group_by(Season) %>%
  summarise(Count = n(), .groups = 'drop')

# Group by month and summarize total crime count per month
crime_by_month <- crime_data2 %>%
  group_by(Month) %>%
  summarise(Count = n(), .groups = 'drop')

# Plot crime counts by season to identify seasonal patterns
ggplot(crime_by_season, aes(x = Season, y = Count, fill = Season)) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Seasonal Crime Patterns", x = "Season", y = "Crime Count") +
  theme_minimal()



# Ensure date columns are in proper date format
crime_data2$START_DATE <- as.Date(crime_data2$START_DATE)

# Extract the year and month from the START_DATE column
crime_data2$YearMonth <- format(crime_data2$START_DATE, "%Y-%m")

# Filter the data to include only from January 2023
crime_data_filtered <- crime_data2 %>%
  filter(YearMonth >= "2022-12")

# Group by YearMonth and count the number of offenses
offense_by_month <- crime_data_filtered %>%
  group_by(YearMonth) %>%
  summarise(Count = n(), .groups = 'drop')

# Plot the temporal analysis (Offenses by Month) with flipped coordinates
ggplot(offense_by_month, aes(x = YearMonth, y = Count, group = 1)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  labs(title = "Number of Offenses Over Time (Starting from Jan 2023)", x = "Year-Month", y = "Offense Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip() +  # Flip the x and y axes
  theme_minimal()



```


# 4.8) shift vs offence 

```{r}
# Summarize the count of offenses by SHIFT and OFFENSE
offense_by_shift_summary <- crime_data2 %>%
  group_by(SHIFT, OFFENSE) %>%
  summarise(Total_Offenses = n(), .groups = 'drop')
# here we replace the barplot with pie chart

ggplot(offense_by_shift_summary, aes(x = 2, y = Total_Offenses, fill = OFFENSE)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar(theta = "y") + 
  xlim(0.5, 2.5) + 
  theme_void() +  
  facet_wrap(~ SHIFT) +  
  labs(title = "Doughnut Chart: Offense Distribution by Shift") +
  scale_fill_brewer(palette = "Set3") + 
  theme(legend.position = "bottom") 
``` 

```{r}
library(dplyr)
library(ggplot2)

offense_summary <- crime_data2 %>%
  group_by(OFFENSE) %>%
  summarise(Count = n(), .groups = 'drop') %>%
  arrange(desc(Count))

offense_summary

```

```{r}
# Summarize total offenses by ward
offense_total_by_ward <- crime_data2 %>%
  group_by(WARD) %>%
  summarise(Total_Offenses = n())

# View the summarized data
print(offense_total_by_ward)
```


```{r}
# Summarize offense counts by Shift
offense_by_shift <- crime_data2 %>%
  group_by(SHIFT, OFFENSE) %>%
  summarise(Count = n(), .groups = 'drop')

# Create a bar plot to visualize offenses by shift
ggplot(offense_by_shift, aes(x = OFFENSE, y = Count, fill = SHIFT)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Offense Types by Shift", x = "Offense Type", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

```



# 4.8) coorrelation--
between numerical variables x,y ,ward, latitude , longitude  

```{r}
# Select numeric columns for correlation analysis
numeric_columns <- crime_data2 %>%
  select(X, Y, LATITUDE, LONGITUDE, PSA, WARD, DISTRICT)

# Check the structure of numeric columns
str(numeric_columns)

```
 
```{r}
# Convert non-numeric columns to numeric (if necessary)
numeric_columns <- crime_data2 %>%
  select(X, Y, LATITUDE, LONGITUDE, PSA, WARD, DISTRICT) %>%
  mutate(across(everything(), ~ as.numeric(.)))

# Check structure to confirm all columns are now numeric
str(numeric_columns)

```
# correlation_matrix -
from the correlation matrix we can say, ward and location(longitude) have highest correlation  

```{r}
# Calculate the correlation matrix
correlation_matrix <- cor(numeric_columns, use = "complete.obs")

# Print the correlation matrix
print(correlation_matrix)

```
```{r}
# Set a CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org/"))

# Load necessary libraries or install if not available
if (!requireNamespace("corrplot", quietly = TRUE)) {
  install.packages("corrplot")
}

```

# 4.8) Correlation plot.

In the correlation plot- we can see the correation between PSA and longitude is near about 0.62, PSA and WARD is about -0.52, district and ward - 0.54.
from this Correlation plot, we can see their is relation between numerical data. 



```{r}

library(corrplot)


# Calculate the correlation matrix
correlation_matrix <- cor(numeric_columns, use = "complete.obs")

# Plot the correlation matrix using corrplot
corrplot(correlation_matrix, method = "color", type = "upper", 
         tl.col = "black", tl.cex = 0.8, number.cex = 0.8,
         addCoef.col = "black", title = "Correlation Matrix of Crime Data")

```
```{r}

library(ggplot2)
library(sf)
library(dplyr)
library(tidycensus)

```


```{r}
library(devtools)

#devtools::install_github("BingoLaHaye/DCmapR")

```



```{r}
library(DCmapR)
#devtools::install_github("BingoLaHaye/DCmapR")
```
# 5) Stastical Test--

Lets start with the Stastical test now

# 5.1) Chi-Square Test Results - (shift and offence type)
chi-square test fir shift and offence type. 
Null Hypothesis (H0): There is no association between Shift and Offense type.

Alternative Hypothesis (H1): There is an association between Shift and Offense type. This indicates that the distribution of offense types is dependent on the time of day (Shift).

The Chi-Square Test Results we have done between Shift and Offence Type. And we got the result as follow 
1) Test Statistic (X-squared): 3258
2) Degrees of Freedom (df): 16
3)  P-value: < 2e-16 - As the P value is very less than 0.05 we can say that, there is a statistically significant association between the time of day (Shift) and the types of offenses recorded in your data. In practical terms, this means that the frequency of different offense types varies significantly depending on whether the incident occurred during the day, evening, or midnight shifts.


WARNING massage- is die to the one of the variable  have less frequency than 5. as we can see that our shift are just divided into 3 parts(DAY,EVENING,MIDNIGHT). 


```{r}
# Load necessary library
library(dplyr)

# Create a contingency table for Shift and Offense
table_shift_offense <- table(crime_data2$SHIFT, crime_data2$OFFENSE)

# Run the Chi-square test
chi_square_shift_offense <- chisq.test(table_shift_offense)

# Print the results
print(chi_square_shift_offense)


```

# 5.2 Fisher-test (shift and offence type)
As we can see that , we have  have less frequency than 5 in the shift. so we have a alter native test called as Fisher-test. 
Reults from the Fisher-Test.

Null Hypothesis (H0): There is no association between Shift and Offense type.

Alternative Hypothesis (H1): There is an association between Shift and Offense type. This indicates that the distribution of offense types is dependent on the time of day (Shift).

we reject the null hypothesis (H0). This indicates that there is a statistically significant association between the time of day (shift) and the type of offense committed.


```{r}
# If your table is small enough
# Create the contingency table
contingency_table_shift_offense <- table(crime_data2$SHIFT, crime_data2$OFFENSE)


fisher_test_result <- fisher.test(contingency_table_shift_offense, simulate.p.value = TRUE)
print(fisher_test_result)
print(fisher_test_result)
```
# 5.3 Chi-squard test( shift vs method)
Null Hypothesis (H0): There is no association between the shift and the method used in offenses (i.e., the distribution of methods is the same across shifts)
Alternative Hypothesis (H1): There is an association between the shift and the method used in offenses (i.e., the distribution of methods varies across shifts).

Test Statistic-
df=4
p value-2e-16


```{r}
# Create a contingency table
contingency_table_shift_method <- table(crime_data2$SHIFT, crime_data2$METHOD)

# first we will do  Chi-squared test
chi_squared_result <- chisq.test(contingency_table_shift_method)



print(chi_squared_result)
```
# 5.4)Fisher-test (shift and method )
p-value is much lower than the common significance level so we reject the null hypothesis and accept the alternative hypotheis test. 


```{r}

# now we are verifing the result with the fisher test-
fisher_result <- fisher.test(contingency_table_shift_method, simulate.p.value = TRUE)
print(fisher_result)
```


```{r}
head(crime_data2)
```
# 5.5) ANOVA- Analysis of Variance,

from the below annova test, ans --  method used to commit offenses has a statistically significant effect on the count of offenses.  

Null Hypothesis (H0): There is no significant effect of the method used (e.g., GUN, OTHERS) on the count of offenses.

Alternative Hypothesis (H1): At least one method has a significant effect on the count of offenses.
The p-value (0.00034) is less than 0.05, so we reject the null hypothesis.



and for the type of offence and count of offence.
Null Hypothesis (H0): There is no significant effect of the type of offense (e.g., ROBBERY, THEFT/OTHER) on the count of offenses.

Alternative Hypothesis (H1): At least one type of offense has a significant effect on the count of offenses.


```{r}
library(dplyr)

# Create a summarized data frame counting occurrences of offenses by SHIFT and METHOD
summary_data <- crime_data2 %>%
  group_by(SHIFT, METHOD, OFFENSE) %>%
  summarise(Count = n(), .groups = 'drop')

# Convert SHIFT and METHOD to factors if not already
summary_data$SHIFT <- as.factor(summary_data$SHIFT)
summary_data$METHOD <- as.factor(summary_data$METHOD)


# Perform ANOVA using Count as the dependent variable
anova_result <- aov(Count ~ SHIFT + METHOD + OFFENSE, data = summary_data)

# Print the summary of ANOVA results
summary(anova_result)

```

p-value < 2.2e-16  significantly less than 0.05 so we reject the null hypothesis and accepet the alternative hypothesis. 

The results suggest that the timing of "GUN" offenses varies significantly by shift, with a greater number of occurrences during MIDNIGHT compared to DAY and EVENING. This finding could have implications for law enforcement and crime prevention strategies, indicating a need for targeted resource allocation during specific times.

 

```{r}
# Load necessary library
library(dplyr)

# Assuming 'data' is your data frame
# Step 1: Filter the data for method "GUN"
gun_data <- crime_data2 %>%
  filter(METHOD == "GUN") %>%
  select(SHIFT)

# Step 2: Create a contingency table
contingency_table <- table(gun_data$SHIFT)

# Print the contingency table
print(contingency_table)

# Step 3: Perform Chi-Square Test
chi_square_result <- chisq.test(contingency_table)

# Step 4: Print Results
print(chi_square_result)

# Step 5: Interpretation
if (chi_square_result$p.value < 0.05) {
  print("Reject the null hypothesis: There is a significant association between the method 'GUN' and the shifts.")
} else {
  print("Fail to reject the null hypothesis: There is no significant association between the method 'GUN' and the shifts.")
}

```

let see with disctrict , is . evry disctrcit have same frequence for the method. 
 H0 - every distrcit have same amount of the gun accident happend during 
 H1-- different distrcit have different amount of incident occurs. 
 
results suggest that the use of "GUN" as an offense method varies significantly across different districts. This finding implies that certain districts may experience higher occurrences of gun-related offenses than others. Understanding these variations can inform law enforcement strategies and resource allocation to address crime effectively in areas where gun offenses are more prevalent.

 
```{r}
gun_data <- crime_data2 %>%
  filter(METHOD == "GUN") %>%
  select(DISTRICT)


# Step 2: Create a contingency table
contingency_table <- table(gun_data$DISTRICT)

# Print the contingency table
print(contingency_table)

# Step 3: Perform Chi-Square Test
chi_square_result <- chisq.test(contingency_table)

# Step 4: Print Results
print(chi_square_result)

# Step 5: Interpretation
if (chi_square_result$p.value < 0.05) {
  print("Reject the null hypothesis: There is a significant association between the method 'GUN' and the DISTRICT.")
} else {
  print("Fail to reject the null hypothesis: There is no significant association between the method 'GUN' and the DISTRICT.")
  
  
  
district_offense_counts <- gun_data %>%
  group_by(DISTRICT) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count))  # Sort in descending order

# Step 3: Print the results
print(district_offense_counts)

# Step 4: Identify the district with the most offenses
most_offenses_district <- district_offense_counts[1, ]
print(paste("The district with the most 'GUN' offenses is:", most_offenses_district$DISTRICT, "with", most_offenses_district$Count, "offenses."))
}

```

  

```{r}
district_offense_counts <- gun_data %>%
  group_by(DISTRICT) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count))  # Sort in descending order

# Step 3: Print the results
print(district_offense_counts)

# Step 4: Identify the district with the most offenses
most_offenses_district <- district_offense_counts[1, ]
print(paste("The district with the most 'GUN' offenses is:", most_offenses_district$DISTRICT, "with", most_offenses_district$Count, "offenses."))
```
 The district with the most 'GUN' offenses is: 6 with 770 offenses."
 
# 5.6) Chi_square test ( Offence Type VS Method )
```{r}
# Load necessary library
library(dplyr)
contingency_table <- table(crime_data2$OFFENSE, crime_data2$METHOD)

print(contingency_table)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
if (chi_square_result$p.value < 0.05) {
  print("Reject the null hypothesis: There is a significant association between the offense type and the method used.")
} else {
  print("Fail to reject the null hypothesis: There is no significant association between the offense type and the method used.")
}
```




```{r}
offense_count <- crime_data2 %>%
  group_by(NEIGHBORHOOD_CLUSTER, OFFENSE) %>%
  summarise(Count = n(), .groups = 'drop')

# Perform ANOVA
anova_result <- aov(Count ~ NEIGHBORHOOD_CLUSTER + OFFENSE, data = offense_count)

# Display the summary of the ANOVA test
summary(anova_result)

# Perform Tukey's HSD test for NEIGHBORHOOD_CLUSTER
tukey_cluster <- TukeyHSD(aov(Count ~ NEIGHBORHOOD_CLUSTER, data = offense_count))
#print(tukey_cluster)

# Perform Tukey's HSD test for OFFENSE
tukey_offense <- TukeyHSD(aov(Count ~ OFFENSE, data = offense_count))
#print(tukey_offense)
```

Chisquare test fri catagorical values
#offence vs method 

```{r}
# Load necessary library
library(dplyr)

# Step 1: Create Contingency Table
contingency_table <- table(crime_data2$OFFENSE, crime_data2$METHOD)

# Print the contingency table
print("Contingency Table:")
print(contingency_table)

# Step 2: Perform Chi-Square Test
chi_square_result <- chisq.test(contingency_table)

# Step 3: Print Results
print("Chi-Square Test Results:")
print(chi_square_result)

# Step 4: Interpretation
if (chi_square_result$p.value < 0.05) {
  print("Reject the null hypothesis: There is a significant association between the offense type and the method used.")
} else {
  print("Fail to reject the null hypothesis: There is no significant association between the offense type and the method used.")
}
```

above we can see that  the relationship between offense types and the methods used in reported crimes. . The table displays counts for different offense categories (e.g., Arson, Assault with Dangerous Weapon, etc.) across three methods (GUN, KNIFE, OTHERS)

p-value significantly less than 0.05, leading to the rejection of the null hypothesis. This indicates that there is a significant association between the offense type and the method used to commit those offenses. 
 
the choice of method (GUN, KNIFE, OTHERS) varies depending on the type of offense.


# 5.7)#CHISQUARE TEST BETWEEN THE OFFENCE AND WARD --- 

A Chi-Square test was conducted to determine whether there was a statistically significant association between the ward and the offense type.

The test resulted in a Chi-Square statistic (X-squared) of 3590.4 with 56 degrees of freedom, and a p-value of less than 2.2e-16.

We first examined the distribution of offenses across various wards. Wards 1, 2, and 5 showed relatively higher offense counts, while Ward 3 had the lowest.
A contingency table was created to show the frequency of different offense types (e.g., Assault, Burglary, Robbery) across each ward.

Since the p-value was much smaller than 0.05, we rejected the null hypothesis, indicating that there is a significant association between the ward and the type of offense. In other words, different wards have distinct crime patterns, and the distribution of offense types is not random across the wards.

```{r}
# Step 1: Summarize the data
summary(crime_data2$WARD)
summary(crime_data2$OFFENSE)

# Step 2: Create a contingency table
ward_offense_table <- table(crime_data2$WARD, crime_data2$OFFENSE)

# Step 3: Visualize the data
# Use bar plot to visualize the distribution of offenses across wards
barplot(ward_offense_table, beside = TRUE, legend = TRUE, 
        col = rainbow(nrow(ward_offense_table)), 
        main = "Distribution of Offenses Across Wards",
        xlab = "Offense Type", ylab = "Count")

# Heatmap to visualize relationship between Ward and Offense
heatmap(ward_offense_table, Rowv = NA, Colv = NA, 
        col = heat.colors(256), scale = "column", 
        main = "Heatmap of Ward vs Offense")

# Step 4: Perform Chi-Square Test
chi_square_ward_offense <- chisq.test(ward_offense_table)
# Step 5: Print the results
print("Contingency Table:")
print(ward_offense_table)

print("Chi-Square Test Results:")
print(chi_square_ward_offense)


# Step 6: Interpretation
if (chi_square_ward_offense$p.value < 0.05) {
  print("Reject the null hypothesis: There is a significant association between the ward and the offense type.")
} else {
  print("Fail to reject the null hypothesis: There is no significant association between the ward and the offense type.")
}

```
# 5.8)ANNOVA for district and ward 
let check there is signofocant relation betweent the district and ward 

```{r}
# Load necessary library
library(dplyr)
# Perform ANOVA
anova_result <- aov(DISTRICT ~ factor(WARD), data = crime_data2)

# Print the summary of ANOVA result
summary(anova_result)
# Extract the p-value for the Ward variable (typically found in the first row of the ANOVA summary)
p_value <- summary(anova_result)[[1]][["Pr(>F)"]][1]

# Step 3: Use an if statement to check the p-value
if (p_value < 0.05) {
  print("Reject the null hypothesis: There are significant differences in crime rate across different Wards.")
} else {
  print("Fail to reject the null hypothesis: There are no significant differences in crime rate across different Wards.")
}
```
p-value that is less than 0.05 indicate that There are statistically significant differences in crime rates across different wards, meaning some wards experience higher or lower crime rates than others.

p-value that is less than 0.05 indicate that There are statistically significant differences in crime rates across different wards, meaning some wards experience higher or lower crime rates than others.


# Maps Of the DC 


```{r}
library(tigris)
dc_wards <- state_legislative_districts(state = "dc", class='sf')
```
# Gun offence-
In analyzing the crime data, we identified three primary methods of offense: Gun, Knife, and Other (which includes hands, sticks, rods, etc.). It is evident that gun-related offenses are associated with severe injuries and fatalities more frequently than other methods. Our findings indicate a strong correlation between gun offenses and specific wards and districts.

Interestingly, the analysis revealed no significant relationship between knife offenses and the corresponding wards or districts. This suggests that knife-related incidents are relatively isolated or less prevalent in comparison to gun offenses.

Given this insight, it is crucial to focus on the areas with high incidences of gun violence. By identifying these wards, law enforcement can strategically increase police surveillance and potentially establish additional police stations in hotspots. This proactive approach aims to enhance public safety and reduce gun-related incidents.


```{r}
# Load necessary libraries
library(tidyverse)
library(sf)
library(dplyr)
library(ggplot2)

gun_offenses <- crime_data2 %>%
  filter(METHOD == "GUN") %>%
  group_by(WARD) %>%
  summarize(gun_offense_count = n())

# Ensure the WARD column in gun_offenses is a character and match the format of NAMELSAD
gun_offenses <- gun_offenses %>%
  mutate(WARD = paste("Ward", as.character(WARD)))  # Add "Ward " prefix

# Merge dc_wards with gun offense counts
dc_ward_gun_offenses <- dc_wards %>%
  left_join(gun_offenses, by = c("NAMELSAD" = "WARD"))

# Check the structure of the merged data to confirm join
print(dim(dc_ward_gun_offenses))  # Print dimensions
print(head(dc_ward_gun_offenses))  # Preview the first few rows

# Plot the map
ggplot(dc_ward_gun_offenses) +
  geom_sf(aes(fill = gun_offense_count), color = "black") +  # Color by gun offense count
  geom_sf_label(aes(label = paste0(NAMELSAD, "\n", gun_offense_count)), size = 3) +  # Add ward labels
  scale_fill_gradient(low = "lightyellow", high = "red", na.value = "gray") +  # Color scale
  labs(title = "Gun Offenses by Ward in Washington, DC", fill = "Gun Offenses") +
  theme_minimal() +
  theme(panel.grid = element_blank())


```
```{r}
gun_offenses
#dc_ward_gun_offenses
#crime_data2

```
```{r}
# get tge boundries by districts.
dc_districts <- state_legislative_districts(state = "dc", class = 'sf')




```




# MAP conculsion by- 
# 1) Offenses by Ward in Washington, DC (Midnight Shift & Robbery)--

Ward-7 is red hot shopt regarding the offences in midnight for robbery. 
followed by ward 2. 
The least offences or safe area in the washigton is ward-3 . 


```{r}
# Load necessary libraries
library(tidyverse)
library(sf)
library(dplyr)
library(ggplot2)

# Filter the crime_data2 dataset for gun offenses during the midnight shift and robbery offenses
gun_offenses <- crime_data2 %>%
  filter(METHOD == "GUN", SHIFT == "MIDNIGHT", OFFENSE == "ROBBERY") %>%  # Add filters for shift and offense
  group_by(WARD) %>%
  summarize(gun_offense_count = n(), .groups = 'drop')  # Count occurrences

# Ensure the WARD column in gun_offenses is a character and match the format of NAMELSAD
gun_offenses <- gun_offenses %>%
   mutate(WARD = paste("Ward", as.character(WARD)))  # Add "Ward " prefix if needed

# Merge dc_wards with gun offense counts
dc_ward_gun_offenses <- dc_wards %>%
  left_join(gun_offenses, by = c("NAMELSAD" = "WARD"))


# Plot the map
ggplot(dc_ward_gun_offenses) +
  geom_sf(aes(fill = gun_offense_count), color = "black") +  # Color by gun offense count
  geom_sf_label(aes(label = paste0(NAMELSAD, "\n", gun_offense_count)), size = 3) +  # Add ward labels
  scale_fill_gradient(low = "lightyellow", high = "red", na.value = "gray") +  # Color scale
  labs(title = "Gun Offenses by Ward in Washington, DC (Midnight Shift & Robbery)", fill = "Gun Offenses") +
  theme_minimal() +
  theme(panel.grid = element_blank())

```
# Summary of Findings by map----
Our analysis reveals that Ward 7 is identified as a significant hotspot for midnight robbery offenses, demonstrating the highest concentration of gun-related crimes during this timeframe. This alarming trend necessitates immediate attention from law enforcement agencies to enhance safety and security measures in the area.

Following closely is Ward 2, which also exhibits a notable frequency of gun offenses. The patterns observed in these wards suggest that targeted interventions, such as increased police presence and community outreach initiatives, are essential to mitigate the risks associated with these high-crime zones.

In stark contrast, Ward 3 emerges as the safest area in Washington, DC, exhibiting the least number of gun offenses. This relative security indicates effective community safety practices and law enforcement strategies that could be analyzed and potentially replicated in more troubled wards.

Overall, these findings underscore the importance of strategic resource allocation to combat gun violence effectively. Enhanced surveillance and police visibility in Wards 7 and 2, combined with community engagement programs, could play a crucial role in reducing criminal activity and fostering safer neighborhoods.



# 6 Discussion--

Crime Trends: Our analysis indicates that theft is the predominant type of crime in Washington, DC. Seasonal variations were evident, with significant spikes in crime during the summer months, particularly in July, likely due to increased outdoor activities and population density.

Impact of Events: We observed a rise in crime rates in October 2023, coinciding with the "Rock the Park" event. This correlation highlights the need for enhanced security measures during large public gatherings, as such events can create opportunities for theft and other offenses.

Temporal Patterns: The majority of crimes occurred during the evening hours, suggesting that law enforcement should increase visibility and patrols during these peak times, especially in identified crime hotspots.

Local vs. Tourist Criminality: An important consideration is whether the individuals committing these crimes are local residents or tourists. Given DC's appeal as a tourist destination, understanding the demographics of offenders can inform targeted crime prevention strategies.

Clusters and Wards: In our analysis, we divided Washington, DC, into clusters and wards to better understand regional crime dynamics. Notably, Ward 3 emerged as the safest area, while Wards 2 and 5 recorded significantly higher crime rates. This finding suggests that crime prevention efforts may need to be tailored to specific wards, particularly in areas experiencing elevated crime levels.

Policy Implications and Future Research: The findings underscore the need for targeted policing strategies and community engagement initiatives to address high theft rates and seasonal crime spikes. Future research should investigate the effectiveness of these strategies and explore the demographics of offenders to gain a clearer understanding of crime dynamics in Washington, DC.

Limitations: While this study provides valuable insights, it is important to acknowledge certain limitations. The analysis relies on reported crime data, which may not capture unreported incidents. Additionally, external factors such as changes in policing strategies or socio-economic conditions could influence crime rates and were not accounted for in this analysis. Our dataset also lacks information on the residency status of offenders, limiting our ability to assess whether crimes are committed by local residents or tourists. Future research should consider incorporating qualitative data to provide a more comprehensive view of the factors influencing crime.

# Recommendations
1)Increase Police Surveillance:
  Focus on wards with high gun offense rates to deter criminal activity.
  
2)Establish More Police Stations:
    Consider placing police stations in areas identified as gun offense hotspots to improve               response times and community engagement.

#Analysis Summary
In this analysis, we observed gun offenses related to robbery occurring at midnight in Washington, DC. Based on the data, we found significant insights regarding the distribution of these offenses across different wards:

Gun Offense Trends:

The analysis indicated that gun offenses are notably concentrated in certain wards during the midnight shift.
Ward 7 emerged as a hotspot, showing the highest frequency of gun-related robbery offenses.
Following closely is Ward 2, which also demonstrated a significant number of incidents.
Comparative Safety:

In contrast, Ward 3 was identified as the safest area with the least occurrences of gun offenses, suggesting it may have effective crime prevention measures in place.






# 7 -conclusion for EDA 


This project aimed to investigate crime patterns in Washington, DC, using data from the Open Data DC portal. We focused on several key areas: identifying geographical crime hotspots, determining the most common offenses, analyzing variations in crime rates across different times of day, months, and seasons, and examining the prevalence of offenses involving firearms and knives compared to other methods.
Our analysis revealed that theft is the most common crime, with significant increases during the summer months, particularly in July, and around major public events like "Rock the Park." By categorizing the city into neighborhoods and wards, we found that Ward 3 is the safest area, while Wards 2 and 5 experience notably higher crime rates. This geographic information is essential for law enforcement and community members, as it helps inform resource allocation and safety strategies.
Regarding weapon use, we discovered that most crimes involved either no weapon or other types of weapons, rather than firearms and knives. Furthermore, our analysis indicated that crimes predominantly occur during the evening hours, emphasizing the need for increased police presence during these times to deter criminal activity effectively.
Understanding these crime dynamics is crucial for residents and visitors alike. By offering insights into crime trends and the methods used in these offenses, we hope to empower individuals to make informed decisions about their safety and living environments. Moving forward, ongoing research into the demographics of offenders and the effectiveness of various crime prevention strategies will be vital in promoting a safer Washington, DC for everyone


The analysis aimed to identify patterns in gun-related offenses across different wards and districts in Washington, DC, and to determine any significant associations among variables that could inform targeted interventions. Using a Chi-square test, a significant relationship was found between WARD and OFFENSE METHOD as well as between DISTRICT and OFFENSE METHOD, indicating that certain wards and districts experience a higher frequency of gun-related offenses.

Based on these findings, a recommendation is to increase police surveillance and allocate more resources to the identified high-risk wards and districts. Targeted patrols, community outreach, and preventative measures in these areas could potentially reduce gun-related offenses, enhancing overall public safety in Washington, DC.


```{r}

selected_columns <- c(
  "REPORT_DAT", "SHIFT", "METHOD", "OFFENSE", "WARD","ANC" ,
  "DISTRICT", "PSA", "X", "Y", "LATITUDE", "LONGITUDE", "Season"
)

data <- crime_data2 %>% select(all_of(selected_columns))

str(data)

```
 check the missing values for the model.
```{r}

missing_summary <- sapply(data, function(x) sum(is.na(x)))
print(missing_summary)
```

```{r}
data$REPORT_DAT <- as.Date(data$REPORT_DAT)
```

```{r}
# Load necessary library
library(dplyr)

# Calculate the number of unique values for each column
unique_values <- sapply(data, function(col) length(unique(col)))

# Convert the result to a data frame for better readability
unique_values_df <- data.frame(
  Column = names(unique_values),
  UniqueValues = unique_values
)

# Print the unique values
print(unique_values_df)

```
```{r}
data$REPORT_DAT <- as.Date(data$REPORT_DAT, format = "%Y-%m-%d")

data$Month <- month(data$REPORT_DAT)  
data$Day <- day(data$REPORT_DAT)     

```
# 8 Model for Gun crime predicatiom 
 as we in the EDA crime related to gun by shift most of the crime happen in the midnight followed by evenning and day . So let's find out  


# MODEL BULDING 

convert the data into factors 

```{r}
crime_data
crime_data$REPORT_DAT <- as.Date(crime_data$REPORT_DAT)

data$ANC <- as.factor(data$ANC)
data$Season <- as.factor(data$Season)
data$SHIFT <- as.factor(data$SHIFT)
data$METHOD <- as.factor(data$METHOD)
data$OFFENSE <- as.factor(data$OFFENSE)
data$WARD <- as.factor(data$WARD)
data$DISTRICT <- as.factor(data$DISTRICT)
data$PSA <- as.factor(data$PSA)

```

# **Lebal Encoding**

all our variables are in the categorical formate so we are have to convert it in to levele according to the   
```{r}
# Label Encoding for ordinal variables (e.g., SHIFT, ANC, PSA)
data$SHIFT <- as.integer(factor(data$SHIFT))   # Label Encoding for SHIFT
data$ANC <- as.integer(factor(data$ANC))       # Label Encoding for ANC
data$PSA <- as.integer(factor(data$PSA))       # Label Encoding for PSA

# One-Hot Encoding for nominal variables (e.g., METHOD, OFFENSE, WARD, DISTRICT, SEASON)
data <- cbind(data, model.matrix(~ METHOD + OFFENSE + WARD + DISTRICT + Season - 1, data = data))

# Remove the original categorical columns after one-hot encoding
data <- data[, !names(data) %in% c('SHIFT', 'METHOD', 'OFFENSE', 'WARD', 'DISTRICT', 'Season')]

# Check the updated data structure
str(data)


```


```{r}
head(data)
```

```{r}
library(ggplot2)

# Plot distribution of LATITUDE and LONGITUDE
ggplot(data, aes(x=LATITUDE)) + 
  geom_histogram(bins=30, fill="blue", color="black", alpha=0.7) + 
  labs(title="Distribution of LATITUDE", x="Latitude", y="Frequency")

ggplot(data, aes(x=LONGITUDE)) + 
  geom_histogram(bins=30, fill="green", color="black", alpha=0.7) + 
  labs(title="Distribution of LONGITUDE", x="Longitude", y="Frequency")


```


corr

```{r}
# Compute correlation matrix
cor_matrix <- cor(data[, sapply(data, is.numeric)])

# Plot correlation heatmap
library(corrplot)
corrplot(cor_matrix, method="circle", type="upper", order="hclust", 
         tl.col="black", tl.srt=65)

selected_vars <- data[, c("LATITUDE", "LONGITUDE", "ANC", "PSA", "METHODGUN", "METHODKNIFE")]

cor_matrix <- cor(selected_vars)

library(corrplot)
corrplot(cor_matrix, method="circle", type="upper", order="hclust", 
         tl.col="black", tl.srt=45, diag=FALSE, addCoef.col="black", number.cex=0.7)


png("correlation_plot.png", width=1500, height=1200, res=150)
corrplot(cor_matrix, method="circle", type="upper", order="hclust", 
         tl.col="black", tl.srt=45, diag=FALSE, addCoef.col="black", number.cex=0.7)
dev.off()
cor_matrix <- cor(data[, sapply(data, is.numeric)])

print(cor_matrix)

```

# feature selection 
from the above we have selected the relative features for our models.

```{r}

head(data)

```

from the corr plot we selected the REPORT_DAT Month Day ANC`, `PSA`, `X`, `Y`, `LATITUDE`, `LONGITUDE `OFFENSEASSAULT W/DANGEROUS WEAPON`,  `OFFENSEMOTOR VEHICLE THEFT`, `OFFENSEROBBERY`, 
`WARD2`, `WARD3`, `WARD5`, `WARD6`, `WARD7`,`WARD8`,`SeasonSpring`,`SeasonSummer`,`SeasonWinter`,`METHODGUN for models

beacume , we tried by using all the variables model is not showing accuracy more than 53% . so that model will be regards as null model . 
so we will just focus on relative variables 

```{r}
# Load necessary libraries
library(dplyr)
str(data)
# Select relevant features for modeling from the 'data' dataset
selected_features <- data %>%
  select( `REPORT_DAT`,`Month`,`Day`, `ANC`, `PSA`, `X`, `Y`, `LATITUDE`, `LONGITUDE`, 
         `OFFENSEASSAULT W/DANGEROUS WEAPON`, 
         `OFFENSEMOTOR VEHICLE THEFT`, 
         `OFFENSEROBBERY`, 
         `WARD2`, `WARD3`, `WARD5`, `WARD6`, `WARD7`,`WARD8`,`SeasonSpring`,`SeasonSummer`,`SeasonWinter`,
         `METHODGUN`)

# Remove rows with missing values
data_selected <- na.omit(selected_features)

# Display the first few rows of the selected features DataFrame
head(data_selected)

# Optionally, save the selected features dataset to a new CSV
write.csv(data_selected, "data_selected.csv", row.names = FALSE)

str(data_selected)
```
```{r}

```

# GLM Model- Offence of motor Vehicle Theft 

we first created the GLM model for the theft regarding the motor vehicle . the target varible in this model is OFFENSEMOTOR_VEHICLE_THEFT. 
```{r}
# Renaming columns with spaces or special characters
colnames(data_selected) <- gsub(" ", "_", colnames(data_selected))
colnames(data_selected) <- gsub("/", "_", colnames(data_selected))

# Check the new column names
colnames(data_selected)

nrow(data_selected)

library(ROSE)
balanced_data <- ovun.sample(OFFENSEMOTOR_VEHICLE_THEFT ~ ., 
                             data = data_selected, 
                             method = "over", 
                             N = nrow(data_selected))$data  # Match the sample size


# Check the structure of the balanced data
str(balanced_data)

table(balanced_data$OFFENSEMOTOR_VEHICLE_THEFT)

# Rebuild the logistic regression model with the balanced data
model_balanced <- glm(OFFENSEMOTOR_VEHICLE_THEFT ~ ANC + PSA + X + Y + LATITUDE + LONGITUDE + 
                     WARD2 + WARD3 + WARD5 + WARD6, data = balanced_data, family = binomial)

# Model summary
summary(model_balanced)


# Get predicted probabilities
pred_prob <- predict(model_balanced, type = "response", newdata = balanced_data)

# Convert probabilities to predicted classes (0 or 1) using a threshold of 0.5
pred_class <- ifelse(pred_prob > 0.5, 1, 0)



# Calculate Accuracy
accuracy <- mean(pred_class == balanced_data$OFFENSEMOTOR_VEHICLE_THEFT)
accuracy


library(pROC)

# Compute the ROC curve and AUC
roc_curve <- roc(balanced_data$OFFENSEMOTOR_VEHICLE_THEFT, pred_prob)
auc(roc_curve)

```

```{r}

# Renaming columns with spaces or special characters
colnames(data_selected) <- gsub(" ", "_", colnames(data_selected))  # Replace spaces with underscores
colnames(data_selected) <- gsub("/", "_", colnames(data_selected))  # Replace '/' with underscores

# Check the new column names
colnames(data_selected)

# Check the number of rows in the dataset
nrow(data_selected)

# Load the ROSE package for oversampling
library(ROSE)

# Balance the dataset using oversampling
balanced_data <- ovun.sample(OFFENSEMOTOR_VEHICLE_THEFT ~ ., 
                             data = data_selected, 
                             method = "over", 
                             N = nrow(data_selected))$data  # Match the sample size

# Check the structure of the balanced data
str(balanced_data)

# Check the distribution of the target variable in the balanced dataset
table(balanced_data$OFFENSEMOTOR_VEHICLE_THEFT)

# Build the logistic regression model with the balanced data
model_balanced <- glm(OFFENSEMOTOR_VEHICLE_THEFT ~ ANC + PSA + X + Y + LATITUDE + LONGITUDE + 
                     WARD2 + WARD3 + WARD5 + WARD6, data = balanced_data, family = binomial)

# Model summary
summary(model_balanced)

# Get predicted probabilities
pred_prob <- predict(model_balanced, type = "response", newdata = balanced_data)

# Convert probabilities to predicted classes (0 or 1) using a threshold of 0.5
pred_class <- ifelse(pred_prob > 0.5, 1, 0)

# Calculate Accuracy
accuracy <- mean(pred_class == balanced_data$OFFENSEMOTOR_VEHICLE_THEFT)
print(paste("Accuracy: ", round(accuracy, 3)))

# Load the pROC package for ROC curve and AUC
library(pROC)

# Compute the ROC curve
roc_curve <- roc(balanced_data$OFFENSEMOTOR_VEHICLE_THEFT, pred_prob)

# Compute AUC (Area Under the Curve)
auc_value <- auc(roc_curve)
print(paste("AUC: ", round(auc_value, 3)))

# Plot the ROC curve
plot(roc_curve, main = "ROC Curve for OFFENSEMOTOR_VEHICLE_THEFT Prediction")




```
as GLM give accurecy about 80% with Area under curve 63% . 



```{r}
# Get unique value

# Display unique values for each column
library(dplyr)
library(lubridate)

# Convert REPORT_DAT to Date
data_selected$REPORT_DAT <- as.Date(data_selected$REPORT_DAT, format = "%Y-%m-%d")
# Check for missing data
      sum(is.na(data_selected))

# Drop missing data rows for simplicity
data_selected <- data_selected %>% drop_na()

str(data_selected)

```

Can we predict the method of crime GUN based on variables like shift, ward, district, and offense type?"

# model Method_GUN Random forest

```{r}
# Load necessary libraries
library(caret)
library(pROC)
library(randomForest)
library(ROSE)  # For oversampling if needed

# Set seed for reproducibility
set.seed(123)

# Check if METHODGUN is a factor, convert if necessary
data_selected$METHODGUN <- as.factor(data_selected$METHODGUN)

# Handle imbalance by oversampling (if necessary)
balanced_data <- ovun.sample(METHODGUN ~ ., data = data_selected, method = "over", N = nrow(data_selected))$data

# Split the data into training and test sets (80-20 split)
train_index <- createDataPartition(balanced_data$METHODGUN, p = 0.8, list = FALSE)
train_data <- balanced_data[train_index, ]
test_data <- balanced_data[-train_index, ]

# Train Random Forest model
rf_model <- randomForest(
  METHODGUN ~ ., 
  data = train_data,
  ntree = 100,
  mtry = sqrt(ncol(train_data) - 1),
  importance = TRUE
)

# Predict on training data
train_predictions <- predict(rf_model, train_data, type = "prob")[,2]
train_classes <- predict(rf_model, train_data)

# Predict on test data
test_predictions <- predict(rf_model, test_data, type = "prob")[,2]
test_classes <- predict(rf_model, test_data)

# Calculate AUC for training and test data
roc_train <- roc(train_data$METHODGUN, train_predictions)
roc_test <- roc(test_data$METHODGUN, test_predictions)

auc_train <- auc(roc_train)
auc_test <- auc(roc_test)

# Print AUC values
cat("AUC (Train Data): ", auc_train, "\n")
cat("AUC (Test Data): ", auc_test, "\n")

# Confusion Matrices
cat("\nConfusion Matrix (Train Data):\n")
print(confusionMatrix(train_classes, as.factor(train_data$METHODGUN)))

cat("\nConfusion Matrix (Test Data):\n")
print(confusionMatrix(test_classes, as.factor(test_data$METHODGUN)))

# Plot ROC curves
plot(roc_train, col = "blue", main = "ROC Curve: Train vs Test")
lines(roc_test, col = "red")
legend("bottomright", legend = c("Train", "Test"), col = c("blue", "red"), lty = 1)

```

Model Performance Summary (Random Forest)
The Random Forest model was trained to predict the method of crime by gun based on various factors such as shift, ward, district, and offense type. The performance of the model was evaluated using Train and Test datasets, with the following results:
Accuracy: 95.7%
Sensitivity: 97.1% (high true positive detection rate)
Specificity: 83.3% (slightly lower performance on non-crime detection compared to training)
Kappa: 0.775 (good agreement)
Balanced Accuracy: 90.2%

The model demonstrated excellent performance on both training and test data, with AUC value  and high accuracy metrics.
The test data results were slightly lower, indicating a small drop in performance when predicting unseen data, but still impressive.
The model was able to accurately predict crime methods, with high sensitivity (detecting most crime events) and strong positive predictive value (high likelihood of correct predictions when it predicted a crime method).
Statistical Significance:

McNemar’s Test for both train and test data indicated significant differences in the performance between predicted and actual values (p-value < 0.05), confirming the model's reliability




```{r}
library(ggplot2)


# Feature importance from the Random Forest model
importance_df <- as.data.frame(importance(rf_model))
importance_df$Feature <- rownames(importance_df)

# Plot feature importance
ggplot(importance_df, aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = "forestgreen") +
  coord_flip() +
  ggtitle("Feature Importance (Random Forest)") +
  xlab("Features") +
  ylab("Mean Decrease in Gini") +
  theme_minimal()


# Add predicted class to test data
test_data$Predicted <- test_classes

# Filter misclassified instances
misclassified <- test_data[test_data$Predicted != test_data$METHODGUN, ]




# Add predicted class to test data
test_data$Predicted <- test_classes

# Filter correctly classified instances
correctly_classified <- test_data[test_data$Predicted == test_data$METHODGUN, ]


# Visualize correctly classified proportions
ggplot(correctly_classified, aes(x = Predicted, fill = as.factor(METHODGUN))) +
  geom_bar(position = "dodge") +
  ggtitle("Correctly Classified Instances in Test Data") +
  xlab("Predicted Class") +
  ylab("Count") +
  scale_fill_manual(values = c("green", "blue"), 
                    name = "Actual Class", 
                    labels = c("Class 0", "Class 1")) +
  theme_minimal()


# Visualize misclassified proportions
ggplot(misclassified, aes(x = Predicted, fill = as.factor(METHODGUN))) +
  geom_bar(position = "dodge") +
  ggtitle("Misclassified Instances in Test Data") +
  xlab("Predicted Class") +
  ylab("Count") +
  scale_fill_manual(values = c("red", "blue"), 
                    name = "Actual Class", 
                    labels = c("Class 0", "Class 1")) +
  theme_minimal()




# Add predicted class to test data
test_data$Predicted <- test_classes

# Create a classification status column
test_data$Status <- ifelse(test_data$Predicted == test_data$METHODGUN, "Correctly Classified", "Misclassified")

# Plot for test data
ggplot(test_data, aes(x = Predicted, fill = Status)) +
  geom_bar(position = "dodge") +
  ggtitle("Classification Status in Test Data") +
  xlab("Predicted Class") +
  ylab("Count") +
  scale_fill_manual(values = c("green", "red"), 
                    name = "Status", 
                    labels = c("Correctly Classified", "Misclassified")) +
  theme_minimal()





# Add predicted class to train data
train_data$Predicted <- train_classes

# Create a classification status column
train_data$Status <- ifelse(train_data$Predicted == train_data$METHODGUN, "Correctly Classified", "Misclassified")

# Plot for train data
ggplot(train_data, aes(x = Predicted, fill = Status)) +
  geom_bar(position = "dodge") +
  ggtitle("Classification Status in Train Data") +
  xlab("Predicted Class") +
  ylab("Count") +
  scale_fill_manual(values = c("green", "red"), 
                    name = "Status", 
                    labels = c("Correctly Classified", "Misclassified")) +
  theme_minimal()



```
chart/graph  --
1-  The feature importance from the image shows the following features, ordered by their mean decrease in Gini:
  OFFENSEROBBERY
  OFFENSEASSAULT_W_DANGEROUS_WEAPON
  X
  REPORT_DAT
  LONGITUDE
  LATITUDE
  Y
  Day
  PSA
  Month

2nd bar plot shows count of  correclty classified instances in test data o- near about 6000 and and for class 1 near about 500.




# bagging tree model 
```{r}
# Load necessary libraries
library(caret)
library(ipred)
library(pROC)

# Set seed for reproducibility
set.seed(123)

# Split the data into training and test sets (80-20 split)
train_index <- createDataPartition(data_selected$METHODGUN, p = 0.8, list = FALSE)
train_data <- data_selected[train_index, ]
test_data <- data_selected[-train_index, ]

# Train Bagging Model
bagging_model <- bagging(
  METHODGUN ~ ., 
  data = train_data,
  nbagg = 100  # Number of bootstrapped models
)

# Predict on training data
train_predictions <- predict(bagging_model, train_data, type = "prob")[, 2]
train_classes <- predict(bagging_model, train_data)

# Predict on test data
test_predictions <- predict(bagging_model, test_data, type = "prob")[, 2]
test_classes <- predict(bagging_model, test_data)

# Calculate AUC for training and test data
roc_train <- roc(train_data$METHODGUN, train_predictions)
roc_test <- roc(test_data$METHODGUN, test_predictions)

auc_train <- auc(roc_train)
auc_test <- auc(roc_test)

# Print AUC values
cat("AUC (Train Data): ", auc_train, "\n")
cat("AUC (Test Data): ", auc_test, "\n")

# Confusion Matrices
cat("\nConfusion Matrix (Train Data):\n")
print(confusionMatrix(as.factor(train_classes), as.factor(train_data$METHODGUN)))

cat("\nConfusion Matrix (Test Data):\n")
print(confusionMatrix(as.factor(test_classes), as.factor(test_data$METHODGUN)))


# Test ROC Curve with AUC
plot(
  roc_test,
  col = "red",
  main = "ROC Curve (Test Data)",
  lwd = 2,
  legacy.axes = TRUE,
  xlab = "False Positive Rate",
  ylab = "True Positive Rate"
)
legend(
  "bottomright",
  legend = paste("AUC =", round(auc_test, 2)),
  col = "red",
  lty = 1,
  lwd = 2
)

```
 
 we tried 2nd model bigging tree model . we got same a
 Accuracy: 0.941

This is the proportion of correct predictions to total predictions. The model correctly classifies 94.1% of the instances in the test data, which is high.
Sensitivity (Recall for Class 0): 0.969

This means the model correctly identifies 96.9% of the instances in class 0 (the majority class). This is very good.
Specificity (Recall for Class 1): 0.698

This is the proportion of actual class 1 instances correctly identified by the model. The specificity of 0.698 means that the model correctly identifies 69.8% of class 1 instances. While decent, this is significantly lower than the sensitivity, indicating that the model struggles somewhat to classify class 1.
 
 
 
 
```{r}
# Predict on Test Data using Bagging Model
bagging_predictions <- predict(bagging_model, test_data, type = "prob")[, 2]

# Predict on Test Data using Random Forest Model
rf_predictions <- predict(rf_model, test_data, type = "prob")[, 2]

# Calculate AUC for Bagging Model
library(pROC)
roc_bagging <- roc(test_data$METHODGUN, bagging_predictions)
auc_bagging <- auc(roc_bagging)

# Calculate AUC for Random Forest Model
roc_rf <- roc(test_data$METHODGUN, rf_predictions)
auc_rf <- auc(roc_rf)

# Print AUC values
cat("AUC (Bagging Model): ", auc_bagging, "\n")
cat("AUC (Random Forest Model): ", auc_rf, "\n")


```

 
```{r}
# Bagging Model Confusion Matrix
bagging_classes <- predict(bagging_model, test_data)
cat("\nConfusion Matrix for Bagging Model:\n")
print(confusionMatrix(as.factor(bagging_classes), as.factor(test_data$METHODGUN)))

# Random Forest Model Confusion Matrix
rf_classes <- predict(rf_model, test_data)
cat("\nConfusion Matrix for Random Forest Model:\n")
print(confusionMatrix(as.factor(rf_classes), as.factor(test_data$METHODGUN)))

```


```{r}
# Plot ROC Curves for both models
plot(roc_bagging, col = "green", main = "ROC Curve Comparison: Bagging vs Random Forest")
lines(roc_rf, col = "blue")
legend("bottomright", legend = c("Bagging Model", "Random Forest Model"), col = c("green", "blue"), lty = 1)

```

```{r}
# Accuracy for Bagging Model
bagging_accuracy <- confusionMatrix(as.factor(bagging_classes), as.factor(test_data$METHODGUN))$overall["Accuracy"]
cat("\nAccuracy for Bagging Model: ", bagging_accuracy, "\n")

# Accuracy for Random Forest Model
rf_accuracy <- confusionMatrix(as.factor(rf_classes), as.factor(test_data$METHODGUN))$overall["Accuracy"]
cat("Accuracy for Random Forest Model: ", rf_accuracy, "\n")

# Sensitivity for Bagging Model
bagging_sensitivity <- confusionMatrix(as.factor(bagging_classes), as.factor(test_data$METHODGUN))$byClass["Sensitivity"]
cat("Sensitivity for Bagging Model: ", bagging_sensitivity, "\n")

# Sensitivity for Random Forest Model
rf_sensitivity <- confusionMatrix(as.factor(rf_classes), as.factor(test_data$METHODGUN))$byClass["Sensitivity"]
cat("Sensitivity for Random Forest Model: ", rf_sensitivity, "\n")

# Specificity for Bagging Model
bagging_specificity <- confusionMatrix(as.factor(bagging_classes), as.factor(test_data$METHODGUN))$byClass["Specificity"]
cat("Specificity for Bagging Model: ", bagging_specificity, "\n")

# Specificity for Random Forest Model
rf_specificity <- confusionMatrix(as.factor(rf_classes), as.factor(test_data$METHODGUN))$byClass["Specificity"]
cat("Specificity for Random Forest Model: ", rf_specificity, "\n")

# Balanced Accuracy for Bagging Model
bagging_balanced_accuracy <- confusionMatrix(as.factor(bagging_classes), as.factor(test_data$METHODGUN))$byClass["Balanced Accuracy"]
cat("Balanced Accuracy for Bagging Model: ", bagging_balanced_accuracy, "\n")

# Balanced Accuracy for Random Forest Model
rf_balanced_accuracy <- confusionMatrix(as.factor(rf_classes), as.factor(test_data$METHODGUN))$byClass["Balanced Accuracy"]
cat("Balanced Accuracy for Random Forest Model: ", rf_balanced_accuracy, "\n")

```

```{r}
bagging_metrics <- matrix(NA, nrow = 10, ncol = 4)  # 10 iterations for Bagging
rf_metrics <- matrix(NA, nrow = 10, ncol = 4)  # 10 iterations for Random Forest

```




# **result** 

Summary of Key Comparisons Between Bagging and Random Forest Models:
Accuracy:

The Random Forest model has a higher accuracy (97.2%) compared to the Bagging model (94.1%), indicating that Random Forest performs better on the test set.
Sensitivity:

The Random Forest model has a higher sensitivity (98.9%) for class 0 compared to the Bagging model (96.8%). This means that Random Forest is better at correctly identifying the majority class (class 0).
Specificity:

The Random Forest model has a significantly higher specificity (82.7%) for class 1, compared to the Bagging model's specificity of 69.8%. This shows that the Random Forest model does a better job of correctly identifying class 1 instances.
Kappa:

The Random Forest model has a higher Kappa (0.843), suggesting better overall agreement between predicted and actual values, compared to the Bagging model (0.674).
Balanced Accuracy:

The Random Forest model has a balanced accuracy of 0.908, which is higher than the Bagging model's balanced accuracy of 0.833. This indicates that Random Forest handles both classes more effectively.


no significant difference between the models random forest and Begging tree ,but model 1 have more accury by soem measure .  you can conclude that Model 1 and Model 2 are statistically similar in terms of accuracy based on the current data and metrics used.
he Random Forest model performs excellently on the training data with perfect accuracy and high sensitivity. On the test data, the model's performance is still strong but slightly lower, particularly in specificity. The drop in specificity on the test data suggests some challenges in correctly identifying negative cases (non-crime method). Despite this, the model remains effective for predicting crime methods with high overall accuracy

# **forecasting** 
Time series analysis for future
 as from models ETS model , we are now forecasting the result for the 2024 amd 2025  by using the ETS model . 

ETS model
```{r}
# Load the required libraries
library(forecast)

# Convert the REPORT_DAT column to Date format (if not already done)
crime_data2$REPORT_DAT <- as.Date(crime_data2$REPORT_DAT)

# Aggregate the data by month and year, count the number of "GUN" offenses per month
monthly_data <- aggregate(REPORT_DAT ~ format(REPORT_DAT, "%Y-%m") + METHOD, data = crime_data2[crime_data2$METHOD == "GUN", ], FUN = length)

# Create a time series for "GUN" offenses per month
# Extract month-year from the aggregated data
monthly_data$Date <- as.Date(paste(monthly_data$`format(REPORT_DAT, "%Y-%m")`, "01", sep = "-"))
monthly_data$Count <- monthly_data$REPORT_DAT  # This is the count of "GUN" offenses per month

# Create the time series from the counts of "GUN" offenses per month
weapon_ts <- ts(monthly_data$Count, start = c(2023, 1), frequency = 12)

# Fit ARIMA model
arima_model <- auto.arima(weapon_ts)

# Fit Exponential Smoothing (ETS) model
ets_model <- ets(weapon_ts)

# Summary of the ETS model
summary(ets_model)

# Forecast for the next 24 months (2024 and 2025)
forecast_ets_monthly <- forecast(ets_model, h = 24)  # 24 months ahead

# Plot the ETS forecast
plot(forecast_ets_monthly, main = "ETS Monthly Forecast for 2024 and 2025")

```



Sarima Model
 
 Sarima forecast for the 2024 and 2024 , from these above model we can predicat the gun offence by time.

```{r}
library(forecast)

# Assuming 'weapon_ts' is your time series object with monthly data (as in your previous context)
# If it's not 'weapon_ts', replace it with your actual time series object

# Fit a SARIMA model (Seasonal ARIMA)
sarima_model <- auto.arima(weapon_ts, seasonal = TRUE, stepwise = TRUE, approximation = FALSE)

# Summary of the SARIMA model
summary(sarima_model)

# Forecast the next 24 months (2024 and 2025)
sarima_forecast <- forecast(sarima_model, h = 24)

# Plot the forecast
plot(sarima_forecast, main = "SARIMA Forecast for 2024 and 2025")



```
forecasting Crime Trends 

```{r}
library(dplyr)
library(ggplot2)
library(forecast)

# Assuming `data_selected` contains latitude and longitude for each crime event
# Create YearMonth and Region identifier for geospatial aggregation
data_selected <- data_selected %>%
  mutate(YearMonth = format(REPORT_DAT, "%Y-%m"),  # Create YearMonth
         Region = paste0("Lat_", round(LATITUDE, 1), "_Lon_", round(LONGITUDE, 1)))  # Create a region identifier

# Aggregate crime data by YearMonth and Region
crime_geo_data <- data_selected %>%
  group_by(YearMonth, Region) %>%
  summarise(Crimes = n())  # Count of crimes per region and time period

# Convert aggregated data to time series for a specific region (example: first region)
crime_ts <- ts(crime_geo_data$Crimes, start = c(2023, 1), frequency = 12)

# Fit the SARIMA model
sarima_model <- auto.arima(crime_ts, seasonal = TRUE)

# Forecast the next 24 months (2024 and 2025) using SARIMA
sarima_forecast <- forecast(sarima_model, h = 24)

# Plot the SARIMA forecast
plot(sarima_forecast, main="SARIMA Forecast for 2024 and 2025")

# Summary for the SARIMA forecast
cat("\nSummary of SARIMA Forecast:\n")
cat("The SARIMA forecast for the next 24 months (2024 and 2025) shows trends and seasonality. The forecast indicates", 
    ifelse(sarima_forecast$mean[1] > sarima_forecast$mean[length(sarima_forecast$mean)], 
           "a decreasing", "an increasing"), 
    "pattern in the data. The forecasted values demonstrate significant variability in crime counts across different months, with higher levels of uncertainty towards the end of the forecast period.\n")

# Fit the ETS model (using crime_ts data)
ets_model_monthly <- ets(crime_ts)

# Summary of the ETS model
summary(ets_model_monthly)

# Forecast for the next 24 months (2024 and 2025) using ETS
forecast_ets_monthly <- forecast(ets_model_monthly, h = 24)

# Plot the ETS forecast
plot(forecast_ets_monthly, main="ETS Monthly Forecast for 2024 and 2025")

# Summary for the ETS forecast
cat("Summary of ETS Forecast:\n")
cat("The forecast for the next 24 months (2024 and 2025) shows a trend of", 
    ifelse(forecast_ets_monthly$mean[1] > forecast_ets_monthly$mean[length(forecast_ets_monthly$mean)], 
           "declining", "increasing"), 
    "values. The forecast indicates possible seasonality, with variations in the crime count across months. The confidence intervals indicate high uncertainty during the forecast period, especially for months farther in the future.\n")

# Create a data frame for the forecasted values and dates (for plotting with ggplot2)
forecast_data <- data.frame(
  Date = seq(as.Date("2024-01-01"), by = "month", length.out = 24),
  Forecast = sarima_forecast$mean
)

# Plot using ggplot2 with custom x-axis labels
ggplot(forecast_data, aes(x = Date, y = Forecast)) +
  geom_line() +
  geom_point() +
  labs(title = "Crime Forecast for 2024-2025 (Geospatial)", x = "Date", y = "Forecasted Crime Count") +
  scale_x_date(labels = scales::date_format("%b %Y"), breaks = "3 months") +  # Adjust breaks here
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate labels for clarity


```


 
