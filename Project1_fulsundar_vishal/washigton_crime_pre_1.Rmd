---
title: "Exploratory Data Analysis on the Washigton crime report "
author: "Vishal Fulsundar, Suraj kapare, Halima Al Balushi, Naiska Buyandalai"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r dropdown="true", echo=TRUE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(warning = F, results = "markup", message = F)
# knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 

``` 

# 1)Introduction---- 

Introduction:
In our project, we aim to investigate crime patterns in Washington, DC, utilizing data from the Open Data DC portal. The dataset, "Crime Incidents in 2023," comprises 34,215 observations detailing reported crimes, including information on the type of offense, time of occurrence, geographic location, and methods involved.
As individuals, especially students, consider buying a house, renting an apartment, relocating, or traveling, one of the first questions that arises is, "Is it safe?" This concern for safety drives many to seek out neighborhoods with low crime rates before evaluating other factors such as price, amenities, and accessibility. Therefore, understanding crime dynamics is crucial not only for law enforcement but also for residents and potential newcomers to the area.
The primary focus of our analysis is to explore crime density across different areas of Washington, DC, identifying neighborhoods that experience the highest levels of criminal activity. This exploration aims to uncover patterns that can inform law enforcement resource allocation and guide community safety initiatives aimed at crime prevention. By providing insights into crime trends, we hope to empower individuals and families to make informed decisions regarding their living environments.


# 2)Data Sources:

Crime Incidents in Washington, DC (2023):
Link: Crime Incidents in 2023
Description: This dataset serves as the primary source for our analysis, encompassing 34,215 reported crime incidents in Washington, DC. It includes critical information such as the type of offense, time of occurrence, geographic location, and methods involved. This comprehensive dataset is essential for examining crime patterns and densities across different neighborhoods
 
 Our Main data frame named- Crime_data have records around 34215observation including the 25 variables. 
 The data set(crime_data) give overiew  of the potensial varibles in the dataset.These varible give /provide the details view of the each incident,enabling us to analyze across  dimensions like geographical location, type of offense, time of occurrence, and associated methods as well as timing of the occurence betwnn the shift(day,evening,midnight).
 ex 1)  Type of offence-  nature of the crime, such as theft, assault, or vandalism, allowing us to categorize and compare crime types
 2) Shift of occurence- shift the crime happened, which can reveal temporal patterns or peak crime times.
 3)Method Involved: Provides information on any specific method or weapon(knief,gun,other) used in the offense 
 4)District or Area: The specific district or police jurisdiction involved,

```{r dropdown="true", echo=TRUE}
 
crime_data <- read.csv("C:/Users/DELL/Documents/GitHub/crime-analysis-washington-dc-2023/Crime_Incidents_in_2023.csv")
head(crime_data)
str(crime_data)
```

Neighborhood Clusters:
Link: Neighborhood Clusters
Description: This dataset provides a mapping of numerical identifiers for neighborhoods to more recognizable names, such as Columbia Heights, Georgetown, and Dupont Circle. By integrating this data, we enhance the interpretability of our analysis, making it easier for stakeholders to understand the geographic context of crime data.

Cluster   has 46 observations and 2 variables and  data frame  represent neighborhood clusters in Washington, DC. 
NBH_NAMES: This column contains the full names of the neighborhoods within each cluster.
NAME: This column contains the cluster identifier (e.g., "Cluster 16", "Cluster 41"), which is a unique numerical label for each neighborhood cluster. 

```{r dropdown="true", echo=TRUE}
library(dplyr)
cluster <- read.csv("C:/Users/DELL/Documents/GitHub/crime-analysis-washington-dc-2023/Neighborhood_Clusters.csv")

cluster <- cluster %>%  select(NAME,NBH_NAMES)
head(cluster)
str(cluster)
```



DC Police Stations:
Link: DC Police Stations
Description: This dataset contains the locations of police stations across Washington, DC. Incorporating this information allows us to analyze the proximity of law enforcement resources to neighborhoods with varying crime rates, offering insights into potential coverage gaps.

Dc police station data frame have  dataframe has 15 observations and 23 variables,
some variables for examples are Zip code , address, lattitude, longitude ,Type, name. 


```{r}
# Reading Police Stations data
police_stations <- read.csv("C:/Users/DELL/Documents/GitHub/crime-analysis-washington-dc-2023/Police_Stations.csv")

head(police_stations)
str(police_stations)

```

# 2.1) Library----
WE have loaded the required library here 
 ggplot2: Used for creating static visualizations, such as line graphs, bar charts, and scatter     plots.
  
dplyr: A data manipulation package that provides functions to filter, arrange, and summarize         data.
  
lubridate: Simplifies date and time manipulation, making it easier to work with date formats,     extract time components, and perform calculations.

sf: Provides support for handling spatial data, allowing you to analyze and visualize geographic data.

leaflet: Used for creating interactive maps, often integrating spatial data from the sf package.

```{r}
# 2) load the require library 

library(ggplot2)
library(dplyr)
# ggplot library for graphs 
library(lubridate) # fro thr data manupulation 
library(sf)
library(leaflet) 
```
# 2.2) data collection--
from the is.null operator , we got that there is no record in the OCTO_RECORD_ID column. total 34215 records are missing from the records.

DISTRICT: 285 records are missing district details.
PSA (Police Service Area): 298 records lack PSA information, which might impact analyses that involve crime distribution across service areas.
CENSUS_TRACT: 16 records are missing census tract data, which could slightly affect demographic or geographic analyses at the census level.
OCTO_RECORD_ID: All 34,215 records are missing from the recoreds . 
so, we have to remove all these recoreds for the eda. From the column named OCTO_RECORD_ID al 34,215 recoreds are missing, so we have to remove this column. We do not require that column. 
del.
 

```{r}
colnames(crime_data)

colSums(is.na(crime_data))

# Check for duplicate rows from the data set 
duplicates <- crime_data[duplicated(crime_data), ]
nrow(duplicates)

head(crime_data)

```

Now, we going to  check the indual column null values, 
PSA (Police Service Area): A total of 33,917 records have non-missing values, indicating that a significant number of incidents have an associated PSA.
CENSUS_TRACT: 34,199 records are populated, suggesting that the majority of entries include census tract information.
DISTRICT: 33,930 records are complete, showing that most incidents are assigned to a specific district.
WARD: There are 5 missing records in this column, which means that only a small fraction of incidents lack associated ward information.
Overall, the dataset is mostly complete for key geographic identifiers, with the exception of the WARD column, which has a minimal number of missing values. This completeness enhances the reliability of our analysis related to crime patterns in Washington, DC.

```{r}
sum(!is.na(crime_data$PSA))          # Check rows with PSA not missing
sum(!is.na(crime_data$CENSUS_TRACT)) # Check rows with CENSUS_TRACT not missing
sum(!is.na(crime_data$DISTRICT))     # Check rows with DISTRICT not missing
sum(is.na(crime_data$WARD))          # Check rows with WARD missing

```
## Data Preprocessing---


To prepare the data, we performed a series of preprocessing steps including cleaning, handling missing values, and transforming data types

 we have to delete these null values , we had founded null from values ward , (census_tract) (geographical region ) , district,  psa (public safty areas )
To ensure the quality of our dataset, we need to remove the records with null values in the WARD, CENSUS_TRACT, DISTRICT, and PSA.

```{r}
crime_data1 <- crime_data[
    !is.na(crime_data$PSA) & 
    !is.na(crime_data$CENSUS_TRACT) & 
    !is.na(crime_data$DISTRICT) 
    , 
]
nrow(crime_data1)
# now 
```
# check Null Values-

we are geeting the result numeric-0 so that means we don't have a null value in the data set. 

cheking the duplicate values in the dataset. 
```{r}

# check duplicate values 
# Ensure crime_data1 is a data frame
crime_data1 <- as.data.frame(crime_data1)

duplicate_crime_data <- crime_data1[duplicated(crime_data1), ]
nrow(duplicate_crime_data)

head(crime_data1)
```
# 2.3)Duplicate---
 so we don't have duplicate vlaues in the data set also . 
 
now our data set is net and clean . 
we are ready for the data transformation .
now we have to convert some rows of data in to some other formate beacuse of our need .
 we some time trand=sform the solumns as according to our use of that data .
 for more readiblity of the data set , means we have the data formate YY/mm/DD HH/MM/SS, as we want to annalyis the data as month as well as day wise , we have to extract the months and days from this formate. 
 create the new columns Year and month from the column Report_dat. 
 as you can see the 2 columns are newly added at the ends of the data-frame. 

```{r}

head(crime_data1)
crime_data1$REPORT_DAT <- as.Date(crime_data1$REPORT_DAT, formate="%m/%d/%Y")

# extract the year and month also for future plots

crime_data1$Year <- format(crime_data1$REPORT_DAT,"%Y")
crime_data1$Month <- format(crime_data1$REPORT_DAT,"%m")

head(crime_data1)

```
# Reframe Data frame
we can see that , 2 new columns are added at the end of the all columns. now readjust columns, add these columns in the data frame after the REPOST_DAT. 

rearrange the columns and move the columns YEAR and Month after the report year .
 
```{r}
crime_data1 <- crime_data1 %>%
    select(1:which(colnames(crime_data1) == "REPORT_DAT"), Year, Month, everything())

head(crime_data1)
str(crime_data1)
```
#Normalization(Optional):
first thing, for the eda part stastical test, we don;t require normalization prcosess. If we want to make a model on the dataset then pnly we have to do normalzition.  
 
now we have to normalize the data set 
Beacuse ----------
1) uniformality across the featurre- in the data set we have geo location data , date time data ,character data (shift ), numerical data(disctrict ) some pincode , so our data have SACLE  are drastically different , if we don;t normalize , some time anlalysis  will affect 
 
2)  it will help in the improving  the speed in analysis 
3) avoid the feature domination effect some feature are x, y co ordinate values are greater than the ward number , so to avioid these domination we have to noramzalise the data set . 
 
 
```{r}
# select the columns , on whcih we have to process the normalize the data. 
# libriry "dplyr" require for the normalize the data . 
# Select numerical columns that need to be normalized
# Exclude categorical columns like OFFENSE, SHIFT, METHOD, etc.
# Select only numeric columns that should be normalized
numeric_columns <- crime_data1 %>%
  select(X, Y, XBLOCK, YBLOCK, LATITUDE, LONGITUDE) 
# Apply Min-Max normalization for each numeric column
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

# Apply normalization to the numeric columns
normalized_numeric_columns <- as.data.frame(lapply(numeric_columns, normalize))

# Add back the non-numeric columns to the normalized dataset
crime_data2 <- bind_cols(crime_data1 %>% select(-X, -Y, -XBLOCK, -YBLOCK, -LATITUDE, -LONGITUDE), 
                          normalized_numeric_columns)

str(crime_data2)

# view the first 5 rows of the data set 
head(crime_data2)
```
# 3) Summary Stastics--
we examined the structure of the normalized dataset and viewed the first five rows to ensure everything was in order. This normalization step will significantly enhance the quality and reliability of our analysis moving forward.

Now start the EDA- Exploratory Data Analysis
 
summary stastics-----
In this section, we present the summary statistics of the dataset, which provide an overview of the key characteristics of the data. 

```{r}
crime_data2 <- crime_data2 %>% 
  left_join(cluster, by = c("NEIGHBORHOOD_CLUSTER" = "NAME"))
summary(crime_data2)

``` 
# 3.1)key observation. 
1) report date- show the range of the date, when 1st offence dtae started to the last date of the offence date. even report date  information is avaible in the quater wise.

2) most of the varibles are catogica like ward, shift, block, methods. 
 in the next, part we will convert some of the variable into int value. 

now we will check with the count(summanry) wise variable. 

 summary provide the  stastics information for each varible in the dataset. '
 as we can see that, most of the offence happen in the ward 
 total wards - 8 , minimum offence in ward- 2 , maximum offence in the ward 5 with 5569. 
Variability: The number of offenses varies significantly across the wards, with Ward 5 having the highest total offenses and Ward 3 the lowest.
Potential Focus Areas: Wards with higher offense totals may require more resources or strategic interventions to address crime effectively.


```{r}

# Summarize total offenses by ward
offense_total_by_ward <- crime_data2 %>%
  group_by(WARD) %>%
  summarise(Total_Offenses = n())

# View the summarized data
print(offense_total_by_ward)

```
# 3.2)Ward wise total offence-- 
 first we have to know , what type of theft it is normally happening in DC. 
 for refernce
1) Theft F/Auto (Theft from Auto)---
    #This type of theft refers to property stolen from a vehicle. It typically includes items such         as: Personal belongings (e.g., bags, electronics, and clothing)
 2) Motor vehicle theft refers to the unlawful taking or stealing of a vehicle itself. This includes:Cars, trucks, motorcycles, and other motor vehicles

 3) Theft/Other--This category encompasses various types of thefts that do not fit into the more specific categories. It may include:-- item lifting from retail stores, Theft of items from homes
 4) Arson---- is the criminal act of deliberately setting fire to property. It can involve burning buildings, vehicles, or other structures
```{r}
offence_count <- crime_data2 %>%
  group_by(OFFENSE)%>%
  summarise(count=n())

print(offence_count)
```
# 3.3) Offence_type Vs count--
The numbers indicate that Theft from Auto and Motor Vehicle Theft are significant issues, highlighting the importance of vehicle security. and least with the Arson type of the offence. 

 now we are looking the offence by the shift wise with the count wise. mean we will know in which shift most of the offence happen, by which method as well as , which type of the offence. so that we can micro - analyze these things.

EVENING (13148)--- count here is the highest among the three shifts, indicating that more offenses happen during this time. Understanding the distribution of crimes by shift can help in analyzing crime trends and identifying times when specific types of crimes are more prevalent, allowing for targeted interventions
```{r}
# Group by SHIFT and summarize the count of offenses
offence_by_shift <- crime_data2 %>%
  group_by(SHIFT) %>%
  summarise(Count = n(), .groups = 'drop')
# Check the structure of the resulting table
#str(offence_by_shift)
# Print the summary table
print(offence_by_shift)
```

# 3.4)crime count by shift 

count by method --
Least Common Offenses---  The offenses with the lowest counts include various types of arson and sex abuse, predominantly occurring during the midnight and day shifts. Notably, arson incidents are quite rare, especially during evening and midnight hours.
Highest Common Offenses: Conversely, theft-related offenses dominate the statistics, particularly the "THEFT/OTHER" category, with significant occurrences in both the evening and daytime shifts. This indicates that theft crimes are more prevalent, especially during peak hours when more people are likely to be out.

```{r}
# Check if SHIFT is a factor or convert it to one if needed
crime_data2$SHIFT <- as.factor(crime_data2$SHIFT)

library(dplyr)

# Ensure SHIFT is a factor
crime_data2$SHIFT <- as.factor(crime_data2$SHIFT)

offence_by_shift <- crime_data2 %>%
  group_by(SHIFT, OFFENSE) %>%
  summarise(Count = n(), .groups = 'drop') %>%
  arrange(desc(Count))

# Check the structure of the resulting data frame
#str(offence_by_shift)
#head(offence_by_shift)
# Print the summary table
print(offence_by_shift)

```
# 3.4) District Vs Count----
District 3 has the highest total count of offenses with 6612 incidents, indicating it may have higher crime activity or reporting in comparison to others.
District 7 shows the lowest total count at 2883
```{r}
# Summarize the count of offenses by district
OFFENCE_BY_DISTRICT <- crime_data2 %>%
  group_by(DISTRICT) %>%
  summarise(count = n(), .groups = 'drop')  # 'drop' will ungroup after summarizing
# Print the summarized table
print(OFFENCE_BY_DISTRICT)
```
As we know most of the varibles are catagorical - analyze the correlation between categorical columns in  dataset, we can use various methods since correlation typically applies to numerical data.
But the numerical data we have like longitude and lattitude , as well as CNN, Report date , but we don;t require numerical data coraltion of this varible. 

# 4)EDA-- 

# 4.1)crime count by shift....
exploratory data analysis. 

This visualization helps us understand which shift have been most affected by theft incidents, providing insights into ward hotspots.

for eda part we are going to use the library ggplot,dyplr,tidyr,ggplot2.
as we can see, the shift VS shift . evening have slighty high count of offence than day. 


``` {r}

# Crime count based on Shift
library(ggplot2)

offense_by_shift_funnel <- crime_data2 %>%
  group_by(SHIFT) %>%
  summarise(Total_Offenses = n(), .groups = 'drop') %>%
  arrange(desc(Total_Offenses))  # Arrange shifts by number of offenses

# Create funnel chart
ggplot(offense_by_shift_funnel, aes(x = SHIFT, y = Total_Offenses, fill = SHIFT)) +
  geom_col(width = 0.7) +  # Bar chart with narrow bars
  coord_flip() +  # Flip to make it look like a funnel
  geom_text(aes(label = Total_Offenses), hjust = -0.2) +  # Add labels
  scale_y_reverse() +  # Reverse the y-axis to give the funnel effect
  labs(title = "Funnel Chart: Offense Counts by Shift", x = "Shift", y = "Total Offenses") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set2")

```




# 4.2) Distrubution of crime offense

distrubution of the crime offence by  type of the offence 
 
horizontal - bar plot for the offec frequency.
cord flip for more readibility of the plot , plot is not good readable as verticle so flip to horizontal.
  
```{r}
ggplot(crime_data2, aes(x = OFFENSE)) +
  geom_bar(fill = "red") +
  labs(title = "Distribution of Crime Offenses", x = "Count", y = "Offense Type") +
  theme(axis.text.y = element_text(size = 100)) +  
  theme_minimal()  +
 coord_flip()  

``` 

so the most common offence is Theft/other , followed by theft/auto --- 

most offense type in the Washigton D.C is theft , the 1st  other and on 2nd is vhehicle 

# 4.3) Distribution of different offenses by shift----
 
```{r}

# Plot the 
ggplot(crime_data2, aes(x = SHIFT, fill = OFFENSE)) +
  geom_bar(position = "dodge") +  # Use dodge to separate bars for different offenses
  labs(title = "Distribution of Offenses by Shift", x = "Shift", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, size = 10, hjust = 1)) +  # Rotate x-axis labels
  theme_minimal()

```
# 4.4)METHOD OF OFFENCE VS OFFENCE-------

Robbery Offenses: We see a noticeable spike in robbery incidents during midnight, which is pretty significant. In fact, robbery offenses outnumber motor vehicle thefts during this time!

Dangerous Weapons: There’s also a rise in the use of dangerous weapons, particularly in the evening and midnight hours. 

```{r}


ggplot(crime_data2, aes(x = METHOD, fill = OFFENSE)) +
  geom_bar(position = "dodge") +  # Use dodge to separate bars for different offenses
  labs(title = "Distribution of Offenses by Method", x = "Method", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, size = 10, hjust = 1)) +  
  theme_minimal()



```
# 4.5)Distribution of  offenses method  by shift---
Dangerous Weapons-- Gun and knif : There’s also a rise in the use of dangerous weapons, particularly in the evening and midnight hours
 
```{r}


ggplot(crime_data2, aes(x = SHIFT, fill = METHOD)) +
  geom_bar(position = "dodge") +  # Use dodge to separate bars for different methods
  labs(title = "Distribution of Offense Methods by Shift", x = "Shift", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, size = 10, hjust = 1)) +  # Rotate x-axis labels
  theme_minimal()



```
# 4.6) Distrubution of offences bt ward-
As we can see, the most common offence in every ward with the highest pick is theft/auto and theft/oter. people take aways most of the things from your car, is the most common offence. 
But as we can see,In the ward-3 every offence is at its least. 
Highest variabily if the offence is in the ward 1 and 2nd but in ward-7 all offence are nearly in same propostion. 
 
```{r}
# Convert WARD to integer

# Check unique values in the WARD column


crime_data2$WARD <- as.factor(crime_data2$WARD)


ggplot(crime_data2, aes(x = WARD, fill = OFFENSE)) +
  geom_bar(position = "dodge") +  
  labs(title = "Distribution of Offenses by Ward", x = "Ward", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, size = 10, hjust = 10)) +theme_minimal()

```
 
In above graph we can see that  - ward 5  have more offence followed by 2
 
 in the below we want to see crime by month-date s0
  Most of the crime happen in the DC of Summer season. 
 When we moving from the summer to winter season we see sudden drop of offence( may be due to harsh environment-). But when we move forward to the fall some theft increases.
 monthly Crime trend- 
 
 Now as we can see from the 2nd month onward up to the 7th month. There is positive trend happend across the offence. And sfter july there is sudden drop of the offence till december expect 10th month , there is unexpected increasing graph for the oct month. 
#

# 4.7) Seasonal Crime pattern 

```{r}

# Ensure START_DATE is in Date format
crime_data2$START_DATE <- as.Date(crime_data2$START_DATE)

# Extract year and month
crime_data2$Year <- format(crime_data2$START_DATE, "%Y")  # Extracting the year as a string
crime_data2$Month <- format(crime_data2$START_DATE, "%b")  # Extracting abbreviated month names

# Define seasons based on months (northern hemisphere)
crime_data2$Season <- case_when(
  crime_data2$Month %in% c("Dec", "Jan", "Feb") ~ "Winter",
  crime_data2$Month %in% c("Mar", "Apr", "May") ~ "Spring",
  crime_data2$Month %in% c("Jun", "Jul", "Aug") ~ "Summer",
  crime_data2$Month %in% c("Sep", "Oct", "Nov") ~ "Fall",
  TRUE ~ "Unknown"  # Catch-all for any unexpected month values
)

# Group by season and summarize total crime count per season
crime_by_season <- crime_data2 %>%
  group_by(Season) %>%
  summarise(Count = n(), .groups = 'drop')

# Group by month and summarize total crime count per month
crime_by_month <- crime_data2 %>%
  group_by(Month) %>%
  summarise(Count = n(), .groups = 'drop')

# Plot crime counts by season to identify seasonal patterns
ggplot(crime_by_season, aes(x = Season, y = Count, fill = Season)) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Seasonal Crime Patterns", x = "Season", y = "Crime Count") +
  theme_minimal()



# Ensure date columns are in proper date format
crime_data2$START_DATE <- as.Date(crime_data2$START_DATE)

# Extract the year and month from the START_DATE column
crime_data2$YearMonth <- format(crime_data2$START_DATE, "%Y-%m")

# Filter the data to include only from January 2023
crime_data_filtered <- crime_data2 %>%
  filter(YearMonth >= "2022-12")

# Group by YearMonth and count the number of offenses
offense_by_month <- crime_data_filtered %>%
  group_by(YearMonth) %>%
  summarise(Count = n(), .groups = 'drop')

# Plot the temporal analysis (Offenses by Month) with flipped coordinates
ggplot(offense_by_month, aes(x = YearMonth, y = Count, group = 1)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  labs(title = "Number of Offenses Over Time (Starting from Jan 2023)", x = "Year-Month", y = "Offense Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip() +  # Flip the x and y axes
  theme_minimal()



```


# 4.8) shift vs offence 

```{r}
# Summarize the count of offenses by SHIFT and OFFENSE
offense_by_shift_summary <- crime_data2 %>%
  group_by(SHIFT, OFFENSE) %>%
  summarise(Total_Offenses = n(), .groups = 'drop')
# here we replace the barplot with pie chart

ggplot(offense_by_shift_summary, aes(x = 2, y = Total_Offenses, fill = OFFENSE)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar(theta = "y") + 
  xlim(0.5, 2.5) + 
  theme_void() +  
  facet_wrap(~ SHIFT) +  
  labs(title = "Doughnut Chart: Offense Distribution by Shift") +
  scale_fill_brewer(palette = "Set3") + 
  theme(legend.position = "bottom") 
``` 

```{r}
library(dplyr)
library(ggplot2)

offense_summary <- crime_data2 %>%
  group_by(OFFENSE) %>%
  summarise(Count = n(), .groups = 'drop') %>%
  arrange(desc(Count))

offense_summary

```

```{r}
# Summarize total offenses by ward
offense_total_by_ward <- crime_data2 %>%
  group_by(WARD) %>%
  summarise(Total_Offenses = n())

# View the summarized data
print(offense_total_by_ward)
```


```{r}
# Summarize offense counts by Shift
offense_by_shift <- crime_data2 %>%
  group_by(SHIFT, OFFENSE) %>%
  summarise(Count = n(), .groups = 'drop')

# Create a bar plot to visualize offenses by shift
ggplot(offense_by_shift, aes(x = OFFENSE, y = Count, fill = SHIFT)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Offense Types by Shift", x = "Offense Type", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

```



# 4.8) coorrelation--
between numerical variables x,y ,ward, latitude , longitude  

```{r}
# Select numeric columns for correlation analysis
numeric_columns <- crime_data2 %>%
  select(X, Y, LATITUDE, LONGITUDE, PSA, WARD, DISTRICT)

# Check the structure of numeric columns
str(numeric_columns)

```
 
```{r}
# Convert non-numeric columns to numeric (if necessary)
numeric_columns <- crime_data2 %>%
  select(X, Y, LATITUDE, LONGITUDE, PSA, WARD, DISTRICT) %>%
  mutate(across(everything(), ~ as.numeric(.)))

# Check structure to confirm all columns are now numeric
str(numeric_columns)

```
# correlation_matrix -
from the correlation matrix we can say, ward and location(longitude) have highest correlation  

```{r}
# Calculate the correlation matrix
correlation_matrix <- cor(numeric_columns, use = "complete.obs")

# Print the correlation matrix
print(correlation_matrix)

```
```{r}
# Set a CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org/"))

# Load necessary libraries or install if not available
if (!requireNamespace("corrplot", quietly = TRUE)) {
  install.packages("corrplot")
}

```

# 4.8) Correlation plot.

In the correlation plot- we can see the correation between PSA and longitude is near about 0.62, PSA and WARD is about -0.52, district and ward - 0.54.
from this Correlation plot, we can see their is relation between numerical data. 



```{r}

library(corrplot)


# Calculate the correlation matrix
correlation_matrix <- cor(numeric_columns, use = "complete.obs")

# Plot the correlation matrix using corrplot
corrplot(correlation_matrix, method = "color", type = "upper", 
         tl.col = "black", tl.cex = 0.8, number.cex = 0.8,
         addCoef.col = "black", title = "Correlation Matrix of Crime Data")

```
```{r}

library(ggplot2)
library(sf)
library(dplyr)
library(tidycensus)

```


```{r}
library(devtools)

#devtools::install_github("BingoLaHaye/DCmapR")

```



```{r}
library(DCmapR)
#devtools::install_github("BingoLaHaye/DCmapR")
```
# 5) Stastical Test--

Lets start with the Stastical test now

# 5.1) Chi-Square Test Results - (shift and offence type)
chi-square test fir shift and offence type. 
Null Hypothesis (H0): There is no association between Shift and Offense type.

Alternative Hypothesis (H1): There is an association between Shift and Offense type. This indicates that the distribution of offense types is dependent on the time of day (Shift).

The Chi-Square Test Results we have done between Shift and Offence Type. And we got the result as follow 
1) Test Statistic (X-squared): 3258
2) Degrees of Freedom (df): 16
3)  P-value: < 2e-16 - As the P value is very less than 0.05 we can say that, there is a statistically significant association between the time of day (Shift) and the types of offenses recorded in your data. In practical terms, this means that the frequency of different offense types varies significantly depending on whether the incident occurred during the day, evening, or midnight shifts.


WARNING massage- is die to the one of the variable  have less frequency than 5. as we can see that our shift are just divided into 3 parts(DAY,EVENING,MIDNIGHT). 


```{r}
# Load necessary library
library(dplyr)

# Create a contingency table for Shift and Offense
table_shift_offense <- table(crime_data2$SHIFT, crime_data2$OFFENSE)

# Run the Chi-square test
chi_square_shift_offense <- chisq.test(table_shift_offense)

# Print the results
print(chi_square_shift_offense)


```

# 5.2 Fisher-test (shift and offence type)
As we can see that , we have  have less frequency than 5 in the shift. so we have a alter native test called as Fisher-test. 
Reults from the Fisher-Test.

Null Hypothesis (H0): There is no association between Shift and Offense type.

Alternative Hypothesis (H1): There is an association between Shift and Offense type. This indicates that the distribution of offense types is dependent on the time of day (Shift).

we reject the null hypothesis (H0). This indicates that there is a statistically significant association between the time of day (shift) and the type of offense committed.


```{r}
# If your table is small enough
# Create the contingency table
contingency_table_shift_offense <- table(crime_data2$SHIFT, crime_data2$OFFENSE)


fisher_test_result <- fisher.test(contingency_table_shift_offense, simulate.p.value = TRUE)
print(fisher_test_result)
print(fisher_test_result)
```
# 5.3 Chi-squard test( shift vs method)
Null Hypothesis (H0): There is no association between the shift and the method used in offenses (i.e., the distribution of methods is the same across shifts)
Alternative Hypothesis (H1): There is an association between the shift and the method used in offenses (i.e., the distribution of methods varies across shifts).

Test Statistic-
df=4
p value-2e-16


```{r}
# Create a contingency table
contingency_table_shift_method <- table(crime_data2$SHIFT, crime_data2$METHOD)

# first we will do  Chi-squared test
chi_squared_result <- chisq.test(contingency_table_shift_method)



print(chi_squared_result)
```
# 5.4)Fisher-test (shift and method )
p-value is much lower than the common significance level so we reject the null hypothesis and accept the alternative hypotheis test. 


```{r}

# now we are verifing the result with the fisher test-
fisher_result <- fisher.test(contingency_table_shift_method, simulate.p.value = TRUE)
print(fisher_result)
```


```{r}
head(crime_data2)
```
# 5.5) ANOVA- Analysis of Variance,

from the below annova test, ans --  method used to commit offenses has a statistically significant effect on the count of offenses.  

Null Hypothesis (H0): There is no significant effect of the method used (e.g., GUN, OTHERS) on the count of offenses.

Alternative Hypothesis (H1): At least one method has a significant effect on the count of offenses.
The p-value (0.00034) is less than 0.05, so we reject the null hypothesis.



and for the type of offence and count of offence.
Null Hypothesis (H0): There is no significant effect of the type of offense (e.g., ROBBERY, THEFT/OTHER) on the count of offenses.

Alternative Hypothesis (H1): At least one type of offense has a significant effect on the count of offenses.


```{r}
library(dplyr)

# Create a summarized data frame counting occurrences of offenses by SHIFT and METHOD
summary_data <- crime_data2 %>%
  group_by(SHIFT, METHOD, OFFENSE) %>%
  summarise(Count = n(), .groups = 'drop')

# Convert SHIFT and METHOD to factors if not already
summary_data$SHIFT <- as.factor(summary_data$SHIFT)
summary_data$METHOD <- as.factor(summary_data$METHOD)


# Perform ANOVA using Count as the dependent variable
anova_result <- aov(Count ~ SHIFT + METHOD + OFFENSE, data = summary_data)

# Print the summary of ANOVA results
summary(anova_result)

```

p-value < 2.2e-16  significantly less than 0.05 so we reject the null hypothesis and accepet the alternative hypothesis. 

The results suggest that the timing of "GUN" offenses varies significantly by shift, with a greater number of occurrences during MIDNIGHT compared to DAY and EVENING. This finding could have implications for law enforcement and crime prevention strategies, indicating a need for targeted resource allocation during specific times.

 

```{r}
# Load necessary library
library(dplyr)

# Assuming 'data' is your data frame
# Step 1: Filter the data for method "GUN"
gun_data <- crime_data2 %>%
  filter(METHOD == "GUN") %>%
  select(SHIFT)

# Step 2: Create a contingency table
contingency_table <- table(gun_data$SHIFT)

# Print the contingency table
print(contingency_table)

# Step 3: Perform Chi-Square Test
chi_square_result <- chisq.test(contingency_table)

# Step 4: Print Results
print(chi_square_result)

# Step 5: Interpretation
if (chi_square_result$p.value < 0.05) {
  print("Reject the null hypothesis: There is a significant association between the method 'GUN' and the shifts.")
} else {
  print("Fail to reject the null hypothesis: There is no significant association between the method 'GUN' and the shifts.")
}

```

let see with disctrict , is . evry disctrcit have same frequence for the method. 
 H0 - every distrcit have same amount of the gun accident happend during 
 H1-- different distrcit have different amount of incident occurs. 
 
results suggest that the use of "GUN" as an offense method varies significantly across different districts. This finding implies that certain districts may experience higher occurrences of gun-related offenses than others. Understanding these variations can inform law enforcement strategies and resource allocation to address crime effectively in areas where gun offenses are more prevalent.

 
```{r}
gun_data <- crime_data2 %>%
  filter(METHOD == "GUN") %>%
  select(DISTRICT)


# Step 2: Create a contingency table
contingency_table <- table(gun_data$DISTRICT)

# Print the contingency table
print(contingency_table)

# Step 3: Perform Chi-Square Test
chi_square_result <- chisq.test(contingency_table)

# Step 4: Print Results
print(chi_square_result)

# Step 5: Interpretation
if (chi_square_result$p.value < 0.05) {
  print("Reject the null hypothesis: There is a significant association between the method 'GUN' and the DISTRICT.")
} else {
  print("Fail to reject the null hypothesis: There is no significant association between the method 'GUN' and the DISTRICT.")
  
  
  
district_offense_counts <- gun_data %>%
  group_by(DISTRICT) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count))  # Sort in descending order

# Step 3: Print the results
print(district_offense_counts)

# Step 4: Identify the district with the most offenses
most_offenses_district <- district_offense_counts[1, ]
print(paste("The district with the most 'GUN' offenses is:", most_offenses_district$DISTRICT, "with", most_offenses_district$Count, "offenses."))
}

```

  

```{r}
district_offense_counts <- gun_data %>%
  group_by(DISTRICT) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count))  # Sort in descending order

# Step 3: Print the results
print(district_offense_counts)

# Step 4: Identify the district with the most offenses
most_offenses_district <- district_offense_counts[1, ]
print(paste("The district with the most 'GUN' offenses is:", most_offenses_district$DISTRICT, "with", most_offenses_district$Count, "offenses."))
```
 The district with the most 'GUN' offenses is: 6 with 770 offenses."
 
# 5.6) Chi_square test ( Offence Type VS Method )
```{r}
# Load necessary library
library(dplyr)
contingency_table <- table(crime_data2$OFFENSE, crime_data2$METHOD)

print(contingency_table)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
if (chi_square_result$p.value < 0.05) {
  print("Reject the null hypothesis: There is a significant association between the offense type and the method used.")
} else {
  print("Fail to reject the null hypothesis: There is no significant association between the offense type and the method used.")
}
```




```{r}
offense_count <- crime_data2 %>%
  group_by(NEIGHBORHOOD_CLUSTER, OFFENSE) %>%
  summarise(Count = n(), .groups = 'drop')

# Perform ANOVA
anova_result <- aov(Count ~ NEIGHBORHOOD_CLUSTER + OFFENSE, data = offense_count)

# Display the summary of the ANOVA test
summary(anova_result)

# Perform Tukey's HSD test for NEIGHBORHOOD_CLUSTER
tukey_cluster <- TukeyHSD(aov(Count ~ NEIGHBORHOOD_CLUSTER, data = offense_count))
#print(tukey_cluster)

# Perform Tukey's HSD test for OFFENSE
tukey_offense <- TukeyHSD(aov(Count ~ OFFENSE, data = offense_count))
#print(tukey_offense)
```

Chisquare test fri catagorical values
#offence vs method 

```{r}
# Load necessary library
library(dplyr)

# Step 1: Create Contingency Table
contingency_table <- table(crime_data2$OFFENSE, crime_data2$METHOD)

# Print the contingency table
print("Contingency Table:")
print(contingency_table)

# Step 2: Perform Chi-Square Test
chi_square_result <- chisq.test(contingency_table)

# Step 3: Print Results
print("Chi-Square Test Results:")
print(chi_square_result)

# Step 4: Interpretation
if (chi_square_result$p.value < 0.05) {
  print("Reject the null hypothesis: There is a significant association between the offense type and the method used.")
} else {
  print("Fail to reject the null hypothesis: There is no significant association between the offense type and the method used.")
}
```

above we can see that  the relationship between offense types and the methods used in reported crimes. . The table displays counts for different offense categories (e.g., Arson, Assault with Dangerous Weapon, etc.) across three methods (GUN, KNIFE, OTHERS)

p-value significantly less than 0.05, leading to the rejection of the null hypothesis. This indicates that there is a significant association between the offense type and the method used to commit those offenses. 
 
the choice of method (GUN, KNIFE, OTHERS) varies depending on the type of offense.


# 5.7)#CHISQUARE TEST BETWEEN THE OFFENCE AND WARD --- 

A Chi-Square test was conducted to determine whether there was a statistically significant association between the ward and the offense type.

The test resulted in a Chi-Square statistic (X-squared) of 3590.4 with 56 degrees of freedom, and a p-value of less than 2.2e-16.

We first examined the distribution of offenses across various wards. Wards 1, 2, and 5 showed relatively higher offense counts, while Ward 3 had the lowest.
A contingency table was created to show the frequency of different offense types (e.g., Assault, Burglary, Robbery) across each ward.

Since the p-value was much smaller than 0.05, we rejected the null hypothesis, indicating that there is a significant association between the ward and the type of offense. In other words, different wards have distinct crime patterns, and the distribution of offense types is not random across the wards.

```{r}
# Step 1: Summarize the data
summary(crime_data2$WARD)
summary(crime_data2$OFFENSE)

# Step 2: Create a contingency table
ward_offense_table <- table(crime_data2$WARD, crime_data2$OFFENSE)

# Step 3: Visualize the data
# Use bar plot to visualize the distribution of offenses across wards
barplot(ward_offense_table, beside = TRUE, legend = TRUE, 
        col = rainbow(nrow(ward_offense_table)), 
        main = "Distribution of Offenses Across Wards",
        xlab = "Offense Type", ylab = "Count")

# Heatmap to visualize relationship between Ward and Offense
heatmap(ward_offense_table, Rowv = NA, Colv = NA, 
        col = heat.colors(256), scale = "column", 
        main = "Heatmap of Ward vs Offense")

# Step 4: Perform Chi-Square Test
chi_square_ward_offense <- chisq.test(ward_offense_table)
# Step 5: Print the results
print("Contingency Table:")
print(ward_offense_table)

print("Chi-Square Test Results:")
print(chi_square_ward_offense)


# Step 6: Interpretation
if (chi_square_ward_offense$p.value < 0.05) {
  print("Reject the null hypothesis: There is a significant association between the ward and the offense type.")
} else {
  print("Fail to reject the null hypothesis: There is no significant association between the ward and the offense type.")
}

```
# 5.8)ANNOVA for district and ward 
let check there is signofocant relation betweent the district and ward 

```{r}
# Load necessary library
library(dplyr)
# Perform ANOVA
anova_result <- aov(DISTRICT ~ factor(WARD), data = crime_data2)

# Print the summary of ANOVA result
summary(anova_result)
# Extract the p-value for the Ward variable (typically found in the first row of the ANOVA summary)
p_value <- summary(anova_result)[[1]][["Pr(>F)"]][1]

# Step 3: Use an if statement to check the p-value
if (p_value < 0.05) {
  print("Reject the null hypothesis: There are significant differences in crime rate across different Wards.")
} else {
  print("Fail to reject the null hypothesis: There are no significant differences in crime rate across different Wards.")
}
```
p-value that is less than 0.05 indicate that There are statistically significant differences in crime rates across different wards, meaning some wards experience higher or lower crime rates than others.

p-value that is less than 0.05 indicate that There are statistically significant differences in crime rates across different wards, meaning some wards experience higher or lower crime rates than others.


# Maps Of the DC 


```{r}
library(tigris)
dc_wards <- state_legislative_districts(state = "dc", class='sf')
```
# Gun offence-
In analyzing the crime data, we identified three primary methods of offense: Gun, Knife, and Other (which includes hands, sticks, rods, etc.). It is evident that gun-related offenses are associated with severe injuries and fatalities more frequently than other methods. Our findings indicate a strong correlation between gun offenses and specific wards and districts.

Interestingly, the analysis revealed no significant relationship between knife offenses and the corresponding wards or districts. This suggests that knife-related incidents are relatively isolated or less prevalent in comparison to gun offenses.

Given this insight, it is crucial to focus on the areas with high incidences of gun violence. By identifying these wards, law enforcement can strategically increase police surveillance and potentially establish additional police stations in hotspots. This proactive approach aims to enhance public safety and reduce gun-related incidents.


```{r}
# Load necessary libraries
library(tidyverse)
library(sf)
library(dplyr)
library(ggplot2)

gun_offenses <- crime_data2 %>%
  filter(METHOD == "GUN") %>%
  group_by(WARD) %>%
  summarize(gun_offense_count = n())

# Ensure the WARD column in gun_offenses is a character and match the format of NAMELSAD
gun_offenses <- gun_offenses %>%
  mutate(WARD = paste("Ward", as.character(WARD)))  # Add "Ward " prefix

# Merge dc_wards with gun offense counts
dc_ward_gun_offenses <- dc_wards %>%
  left_join(gun_offenses, by = c("NAMELSAD" = "WARD"))

# Check the structure of the merged data to confirm join
print(dim(dc_ward_gun_offenses))  # Print dimensions
print(head(dc_ward_gun_offenses))  # Preview the first few rows

# Plot the map
ggplot(dc_ward_gun_offenses) +
  geom_sf(aes(fill = gun_offense_count), color = "black") +  # Color by gun offense count
  geom_sf_label(aes(label = paste0(NAMELSAD, "\n", gun_offense_count)), size = 3) +  # Add ward labels
  scale_fill_gradient(low = "lightyellow", high = "red", na.value = "gray") +  # Color scale
  labs(title = "Gun Offenses by Ward in Washington, DC", fill = "Gun Offenses") +
  theme_minimal() +
  theme(panel.grid = element_blank())


```
```{r}
gun_offenses
#dc_ward_gun_offenses
#crime_data2

```
```{r}
# get tge boundries by districts.
dc_districts <- state_legislative_districts(state = "dc", class = 'sf')




```




# MAP conculsion by- 
# 1) Offenses by Ward in Washington, DC (Midnight Shift & Robbery)--

Ward-7 is red hot shopt regarding the offences in midnight for robbery. 
followed by ward 2. 
The least offences or safe area in the washigton is ward-3 . 


```{r}
# Load necessary libraries
library(tidyverse)
library(sf)
library(dplyr)
library(ggplot2)

# Filter the crime_data2 dataset for gun offenses during the midnight shift and robbery offenses
gun_offenses <- crime_data2 %>%
  filter(METHOD == "GUN", SHIFT == "MIDNIGHT", OFFENSE == "ROBBERY") %>%  # Add filters for shift and offense
  group_by(WARD) %>%
  summarize(gun_offense_count = n(), .groups = 'drop')  # Count occurrences

# Ensure the WARD column in gun_offenses is a character and match the format of NAMELSAD
gun_offenses <- gun_offenses %>%
   mutate(WARD = paste("Ward", as.character(WARD)))  # Add "Ward " prefix if needed

# Merge dc_wards with gun offense counts
dc_ward_gun_offenses <- dc_wards %>%
  left_join(gun_offenses, by = c("NAMELSAD" = "WARD"))

# Check the structure of the merged data to confirm join
print(dim(dc_ward_gun_offenses))  # Print dimensions
print(head(dc_ward_gun_offenses))  # Preview the first few rows

# Plot the map
ggplot(dc_ward_gun_offenses) +
  geom_sf(aes(fill = gun_offense_count), color = "black") +  # Color by gun offense count
  geom_sf_label(aes(label = paste0(NAMELSAD, "\n", gun_offense_count)), size = 3) +  # Add ward labels
  scale_fill_gradient(low = "lightyellow", high = "red", na.value = "gray") +  # Color scale
  labs(title = "Gun Offenses by Ward in Washington, DC (Midnight Shift & Robbery)", fill = "Gun Offenses") +
  theme_minimal() +
  theme(panel.grid = element_blank())

```
# Summary of Findings by map----
Our analysis reveals that Ward 7 is identified as a significant hotspot for midnight robbery offenses, demonstrating the highest concentration of gun-related crimes during this timeframe. This alarming trend necessitates immediate attention from law enforcement agencies to enhance safety and security measures in the area.

Following closely is Ward 2, which also exhibits a notable frequency of gun offenses. The patterns observed in these wards suggest that targeted interventions, such as increased police presence and community outreach initiatives, are essential to mitigate the risks associated with these high-crime zones.

In stark contrast, Ward 3 emerges as the safest area in Washington, DC, exhibiting the least number of gun offenses. This relative security indicates effective community safety practices and law enforcement strategies that could be analyzed and potentially replicated in more troubled wards.

Overall, these findings underscore the importance of strategic resource allocation to combat gun violence effectively. Enhanced surveillance and police visibility in Wards 7 and 2, combined with community engagement programs, could play a crucial role in reducing criminal activity and fostering safer neighborhoods.



# 6 Discussion--

Crime Trends: Our analysis indicates that theft is the predominant type of crime in Washington, DC. Seasonal variations were evident, with significant spikes in crime during the summer months, particularly in July, likely due to increased outdoor activities and population density.

Impact of Events: We observed a rise in crime rates in October 2023, coinciding with the "Rock the Park" event. This correlation highlights the need for enhanced security measures during large public gatherings, as such events can create opportunities for theft and other offenses.

Temporal Patterns: The majority of crimes occurred during the evening hours, suggesting that law enforcement should increase visibility and patrols during these peak times, especially in identified crime hotspots.

Local vs. Tourist Criminality: An important consideration is whether the individuals committing these crimes are local residents or tourists. Given DC's appeal as a tourist destination, understanding the demographics of offenders can inform targeted crime prevention strategies.

Clusters and Wards: In our analysis, we divided Washington, DC, into clusters and wards to better understand regional crime dynamics. Notably, Ward 3 emerged as the safest area, while Wards 2 and 5 recorded significantly higher crime rates. This finding suggests that crime prevention efforts may need to be tailored to specific wards, particularly in areas experiencing elevated crime levels.

Policy Implications and Future Research: The findings underscore the need for targeted policing strategies and community engagement initiatives to address high theft rates and seasonal crime spikes. Future research should investigate the effectiveness of these strategies and explore the demographics of offenders to gain a clearer understanding of crime dynamics in Washington, DC.

Limitations: While this study provides valuable insights, it is important to acknowledge certain limitations. The analysis relies on reported crime data, which may not capture unreported incidents. Additionally, external factors such as changes in policing strategies or socio-economic conditions could influence crime rates and were not accounted for in this analysis. Our dataset also lacks information on the residency status of offenders, limiting our ability to assess whether crimes are committed by local residents or tourists. Future research should consider incorporating qualitative data to provide a more comprehensive view of the factors influencing crime.

# Recommendations
1)Increase Police Surveillance:
  Focus on wards with high gun offense rates to deter criminal activity.
  
2)Establish More Police Stations:
    Consider placing police stations in areas identified as gun offense hotspots to improve               response times and community engagement.

#Analysis Summary
In this analysis, we observed gun offenses related to robbery occurring at midnight in Washington, DC. Based on the data, we found significant insights regarding the distribution of these offenses across different wards:

Gun Offense Trends:

The analysis indicated that gun offenses are notably concentrated in certain wards during the midnight shift.
Ward 7 emerged as a hotspot, showing the highest frequency of gun-related robbery offenses.
Following closely is Ward 2, which also demonstrated a significant number of incidents.
Comparative Safety:

In contrast, Ward 3 was identified as the safest area with the least occurrences of gun offenses, suggesting it may have effective crime prevention measures in place.






# 7)-conclusion 


This project aimed to investigate crime patterns in Washington, DC, using data from the Open Data DC portal. We focused on several key areas: identifying geographical crime hotspots, determining the most common offenses, analyzing variations in crime rates across different times of day, months, and seasons, and examining the prevalence of offenses involving firearms and knives compared to other methods.
Our analysis revealed that theft is the most common crime, with significant increases during the summer months, particularly in July, and around major public events like "Rock the Park." By categorizing the city into neighborhoods and wards, we found that Ward 3 is the safest area, while Wards 2 and 5 experience notably higher crime rates. This geographic information is essential for law enforcement and community members, as it helps inform resource allocation and safety strategies.
Regarding weapon use, we discovered that most crimes involved either no weapon or other types of weapons, rather than firearms and knives. Furthermore, our analysis indicated that crimes predominantly occur during the evening hours, emphasizing the need for increased police presence during these times to deter criminal activity effectively.
Understanding these crime dynamics is crucial for residents and visitors alike. By offering insights into crime trends and the methods used in these offenses, we hope to empower individuals to make informed decisions about their safety and living environments. Moving forward, ongoing research into the demographics of offenders and the effectiveness of various crime prevention strategies will be vital in promoting a safer Washington, DC for everyone


The analysis aimed to identify patterns in gun-related offenses across different wards and districts in Washington, DC, and to determine any significant associations among variables that could inform targeted interventions. Using a Chi-square test, a significant relationship was found between WARD and OFFENSE METHOD as well as between DISTRICT and OFFENSE METHOD, indicating that certain wards and districts experience a higher frequency of gun-related offenses.

Based on these findings, a recommendation is to increase police surveillance and allocate more resources to the identified high-risk wards and districts. Targeted patrols, community outreach, and preventative measures in these areas could potentially reduce gun-related offenses, enhancing overall public safety in Washington, DC.


```{r dropdown='true',echo=TRUE}

head(crime_data2)


```





```{r dropdown='true',echo=TRUE}

# Drop the specified columns from the dataset
crime_data2 <- subset(crime_data2, select = -c(NBH_NAMES, YearMonth, OBJECTID, OCTO_RECORD_ID, 
                                               XBLOCK, YBLOCK, CENSUS_TRACT, VOTING_PRECINCT, 
                                               BID, START_DATE, END_DATE, NEIGHBORHOOD_CLUSTER, 
                                               BLOCK_GROUP, BLOCK, CCN,X,Y))



# Check the structure and first few rows of the updated dataset
str(crime_data2)
head(crime_data2)


```




```{r dropdown='true',echo=TRUE}
#install.packages("DMwR2")

# Load libraries
library(randomForest)
library(caret)
library(DMwR2)  # For SMOTE (Synthetic Minority Oversampling Technique)

table(crime_data2$METHOD)


```




```{r dropdown='true',echo=TRUE}
# Convert categorical variables to factors (if not already done)
crime_data2$METHOD <- factor(crime_data2$METHOD)
crime_data2$SHIFT <- factor(crime_data2$SHIFT)
crime_data2$OFFENSE <- factor(crime_data2$OFFENSE)
crime_data2$WARD <- factor(crime_data2$WARD)
crime_data2$ANC <- factor(crime_data2$ANC)
crime_data2$DISTRICT <- factor(crime_data2$DISTRICT)
crime_data2$Season <- factor(crime_data2$Season)


sum(is.na(crime_data2))

```





```{r dropdown='true',echo=TRUE}
crime_data2 <- na.omit(crime_data2)

sum(is.na(crime_data2))

str(crime_data2)
nrow(crime_data2)

```





```{r dropdown='true',echo=TRUE}
# Split the data into training (70%) and testing (30%) sets
table(crime_data2$Year)
crime_data2 <- crime_data2[crime_data2$Year == "2023", ]

# Extract the day as numeric using base R
crime_data2$Day <- as.numeric(format(crime_data2$REPORT_DAT, "%d"))
# Apply sine and cosine transformations to the Day column
crime_data2$day_sin <- sin(2 * pi * crime_data2$Day / 31)  # 31 days in a month
crime_data2$day_cos <- cos(2 * pi * crime_data2$Day / 31)

# Check the new columns
head(crime_data2[, c("Day", "day_sin", "day_cos")])


# First, create a mapping for month names to numeric values
month_mapping <- c("Jan" = 1, "Feb" = 2, "Mar" = 3, "Apr" = 4, "May" = 5, "Jun" = 6,
                   "Jul" = 7, "Aug" = 8, "Sep" = 9, "Oct" = 10, "Nov" = 11, "Dec" = 12)

# Apply the mapping to convert the 'Month' column to numeric
crime_data2$Month <- month_mapping[crime_data2$Month]

# Check the structure to confirm the conversion
str(crime_data2$Month)

class(crime_data2$Month)



set.seed(123)  # for reproducibility
trainIndex <- createDataPartition(crime_data2$METHOD, p = 0.7, list = FALSE)
trainData <- crime_data2[trainIndex, ]
testData <- crime_data2[-trainIndex, ]

str(trainData)
str(testData)

```





```{r dropdown='true',echo=TRUE}
#install.packages("smotefamily")
table(trainData$METHOD)






```

```{r dropdown='true',echo=TRUE}
# 
str(crime_data2)
head(crime_data2)

```




```{r dropdown='true',echo=TRUE}

# Load the randomForest package
library(randomForest)


# Set class weights, giving more weight to minority classes
class_weights <- c("GUN" = 10, "KNIFE" = 10, "OTHERS" = 1)

# Tune the Random Forest model
rf_tune <- randomForest(METHOD ~ ., 
                        data = trainData, 
                        ntree = 500, 
                        mtry = 3, 
                        nodesize = 5, 
                        classwt = class_weights)

# Check the tuned model summary
print(rf_tune)



```





```{r dropdown='true',echo=TRUE}

# Confusion Matrix
library(caret)
confusionMatrix(predict(rf_tune, testData), testData$METHOD )

# ROC-AUC using the pROC package
library(pROC)
roc_curve <- roc(testData$METHOD, as.numeric(predict(rf_tune, testData, type = "response")))
plot(roc_curve)


```





```{r dropdown='true',echo=TRUE}
#head(crime_data2)

# Load necessary libraries
library(randomForest)
library(caret)

# Example of splitting data into training and test sets
set.seed(42)  # For reproducibility
trainIndex <- createDataPartition(crime_data2$METHOD, p = 0.8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- crime_data2[trainIndex, ]
testData <- crime_data2[-trainIndex, ]

# Scale the features (excluding the target variable 'METHOD')
train_data_scaled <- train_data
testData_scaled <- testData

# Scaling numerical features in both train and test sets
train_data_scaled[, c("Day", "LATITUDE", "LONGITUDE", "Month")] <- scale(train_data[, c("Day", "LATITUDE", "LONGITUDE", "Month")])
testData_scaled[, c("Day", "LATITUDE", "LONGITUDE", "Month")] <- scale(testData[, c("Day", "LATITUDE", "LONGITUDE", "Month")])

# Ensure 'METHOD' is a factor with the same levels in both training and testing data
train_data_scaled$METHOD <- factor(train_data_scaled$METHOD, levels = c("GUN", "KNIFE", "OTHERS"))
testData_scaled$METHOD <- factor(testData_scaled$METHOD, levels = c("GUN", "KNIFE", "OTHERS"))

# Train the Random Forest model
rf_model1 <- randomForest(METHOD ~ Day + LATITUDE + LONGITUDE + Month + WARD + ANC + OFFENSE, 
                          data = train_data_scaled, 
                          ntree = 500)

# Print the model details
print(rf_model1)

# Make predictions on the test dataset
predictions <- predict(rf_model1, newdata = testData_scaled)

# Create the confusion matrix
confusion_matrix <- table(predictions, testData_scaled$METHOD)
print(confusion_matrix)

# Calculate accuracy of the model
accuracy <- sum(predictions == testData_scaled$METHOD) / nrow(testData_scaled)
cat("Accuracy:", accuracy, "\n")




```

```{r}


# Remove 'Year' column from both trainData and testData
trainData2 <- trainData[, !names(trainData) %in% "Year"]
testData2 <- testData[, !names(testData) %in% "Year"]

# Verify that 'Year' is removed from both datasets
str(trainData2)
str(testData2)


# Set seed for reproducibility
set.seed(123)

# Create a control function for cross-validation
control <- trainControl(method = "cv", number = 5)



# Logistic Regression
logistic_model <- train(
  METHOD ~ .,
  data = trainData2,
  method = "multinom",  # Multinomial logistic regression
  trControl = control)

print(logistic_model)

# Predictions using Logistic Regression
logistic_pred <- predict(logistic_model, testData2)
confusionMatrix(logistic_pred, testData2$METHOD)





```



```{r}
#install.packages("e1071")  # Install e1071 if not already installed
library(e1071)  # Load the library

# Train the SVM model using a radial kernel
model_svm <- svm(METHOD ~ ., data = trainData2, kernel = "radial", cost = 1, scale = TRUE)

# Summary of the model
summary(model_svm)

# Make predictions
predictions <- predict(model_svm, testData2)

# Confusion matrix to evaluate performance
confusion_matrix <- table(Prediction = predictions, Actual = testData2$METHOD)
print(confusion_matrix)



```
```{r}




```






```{r}



```



```{r}



```






```{r}  



crime_data2 <- crime_data2[, !colnames(crime_data2) %in% "NEIGHBORHOOD_CLUSTER"]

# Verify the column is dropped
print(colnames(crime_data2))

offense_count <- table(crime_data2$OFFENSE)

# Display the result

print(offense_count)




```
#new


```{r}
str(crime_data2)
library(corrplot)
head(crime_data2)
str(crime_data2)



# Exclude non-numeric columns and calculate correlation
crime_data_numeric <- crime_data2[, sapply(crime_data2, is.numeric)]
corr_matrix <- cor(crime_data_numeric, use="complete.obs")
# Exclude non-numeric columns and calculate correlation
crime_data_numeric <- crime_data2[, sapply(crime_data2, is.numeric)]
corr_matrix <- cor(crime_data_numeric)

library(corrplot)


corrplot(corr_matrix,method = "number",type="full", number.cex = 1.2, tl.cex = 0.8,addCoef.col = "black",   # Add coefficients as black numbers
         col = colorRampPalette(c("blue", "white", "red"))(200))

```

```{r}

# Identify numeric columns in crime_data2
numeric_cols <- sapply(crime_data2, is.numeric)

# Calculate standard deviations for numeric columns
std_devs <- apply(crime_data2[, numeric_cols], 2, sd, na.rm = TRUE)

# Identify columns with zero standard deviation
zero_sd_cols <- names(std_devs[std_devs == 0])
print(zero_sd_cols)  # Columns causing the issue

# Remove columns with zero variance
crime_data_filtered <- crime_data2[, !(names(crime_data2) %in% zero_sd_cols)]

# Calculate correlation matrix for filtered data
corr_matrix <- cor(crime_data_filtered[, sapply(crime_data_filtered, is.numeric)], use = "complete.obs")

# Print and visualize the correlation matrix
print(corr_matrix)

```



```{r}


# Load the caret package for splitting the data
library(caret)
str(testData)
str(trainData)


# Load necessary library
library(class)

# Check structure of the datasets to identify numeric columns
numeric_cols <- c("Month", "PSA", "LATITUDE", "LONGITUDE", "Day", "day_sin", "day_cos")

# Scale numeric features for train and test data
trainData_scaled <- as.data.frame(lapply(trainData[, numeric_cols], scale))
testData_scaled <- as.data.frame(lapply(testData[, numeric_cols], scale))

# Add the target variable (OFFENSE) back to the scaled training data
trainData_scaled$OFFENSE <- trainData$OFFENSE

# Ensure the target variable is a factor
trainData_scaled$OFFENSE <- as.factor(trainData_scaled$OFFENSE)

# Train the KNN model (e.g., k = 9)
knn_pred <- knn(
  train = trainData_scaled[, numeric_cols], # Numeric features of training data
  test = testData_scaled,                  # Numeric features of testing data
  cl = trainData_scaled$OFFENSE,           # Target variable
  k = 9                                    # Number of neighbors
)

# View the first few predictions
head(knn_pred)

# Evaluate model performance
# Add predictions to test data for comparison
testData$Predicted_OFFENSE <- knn_pred

# Confusion matrix
table(Predicted = testData$Predicted_OFFENSE, Actual = testData$OFFENSE)

 
```
```{r}
# Create confusion matrix
confusion_matrix <- table(Predicted = testData$Predicted_OFFENSE, Actual = testData$OFFENSE)

# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

# Print accuracy
print(paste("Accuracy of the KNN model:", round(accuracy * 100, 2), "%"))

```
# new knn-2




```{r}
head(crime_data2)



head(crime_data2)
class(crime_data2$Month)

```


```{r}
set.seed(123)
library(caret)
trainIndex <- createDataPartition(crime_data2$SHIFT, p = 0.8, list = FALSE)
train_data <- crime_data2[trainIndex, ]
test_data <- crime_data2[-trainIndex, ]

# Remove rows with missing Month values
train_data <- na.omit(train_data)

str(crime_data2)
# Fit a Random Forest Model
library(randomForest)
rf_model <- randomForest(SHIFT ~ Month + Day + WARD + Season + day_sin + day_cos, 
                         data = train_data)

# Check for missing values in the dataset
summary(train_data)




# Check model results
print(rf_model)

# Make predictions on the test set
predictions <- predict(rf_model, newdata = test_data)

# Evaluate the model
confusionMatrix(predictions, test_data$SHIFT)



```



```{r}
# Check for missing values in the dataset
sum(is.na(crime_data2))
crime_data2 <- na.omit(crime_data2)


# Use day_sin and day_cos instead of Day
features <- crime_data2 %>%
  select(day_sin, day_cos, LATITUDE, LONGITUDE, Month,WARD,ANC,METHOD )


target <- crime_data2$OFFENSE



# Split the data into training and testing sets (80% training, 20% testing)
set.seed(42)
trainIndex1 <- createDataPartition(target, p = 0.8, list = FALSE)
train_data1 <- crime_data2[trainIndex1, ]
test_data1 <- crime_data2[-trainIndex1, ]

# Scale the features (optional, helps with some algorithms like SVM)
scaler <- preProcess(train_data1[, c("LATITUDE", "LONGITUDE", "Day", "Month", "WARD","ANC","METHOD")], method = "scale")
train_data_scaled <- predict(scaler, train_data1)
test_data_scaled <- predict(scaler, test_data1)

# Train a Random Forest Model
rf_model1 <- randomForest(OFFENSE ~ Day + LATITUDE + LONGITUDE + Month + WARD+ANC+ METHOD, data = train_data_scaled)

# Print model summary
print(rf_model1)

# Make predictions on the test set
predictions1 <- predict(rf_model1, test_data_scaled)

# Evaluate the model's accuracy and performance
confusionMatrix(predictions1, test_data_scaled$OFFENSE)

# Optional: Check the importance of features in the model
importance(rf_model1)


```





```{r}
#Load necessary libraries
library(dplyr)
library(ggplot2)

# Load the dataset
crime_data <- read.csv("C:/Users/DELL/Documents/GitHub/crime-analysis-washington-dc-2023/Crime_Incidents_in_2023.csv")

# Preprocess the data
# Convert date columns to Date format
crime_data$REPORT_DAT <- as.Date(crime_data$REPORT_DAT, format="%Y/%m/%d")
crime_data$START_DATE <- as.Date(crime_data$START_DATE, format="%Y/%m/%d")
crime_data$END_DATE <- as.Date(crime_data$END_DATE, format="%Y/%m/%d")

# Extract features from datetime columns
crime_data$REPORT_YEAR <- format(crime_data$REPORT_DAT, "%Y")
crime_data$REPORT_MONTH <- format(crime_data$REPORT_DAT, "%m")
crime_data$REPORT_DAY <- format(crime_data$REPORT_DAT, "%d")
crime_data$REPORT_HOUR <- format(crime_data$REPORT_DAT, "%H")

# Convert extracted features to numeric
crime_data$REPORT_YEAR <- as.numeric(crime_data$REPORT_YEAR)
crime_data$REPORT_MONTH <- as.numeric(crime_data$REPORT_MONTH)
crime_data$REPORT_DAY <- as.numeric(crime_data$REPORT_DAY)
crime_data$REPORT_HOUR <- as.numeric(crime_data$REPORT_HOUR)

# Select features and target variable
features <- c("X", "Y", "REPORT_YEAR", "REPORT_MONTH", "REPORT_DAY", "REPORT_HOUR")
target <- "OBJECTID"

# Filter the dataset to include only the selected features and target
crime_data_filtered <- crime_data %>% select(all_of(features), target)

# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(seq_len(nrow(crime_data_filtered)), size = 0.8 * nrow(crime_data_filtered))
train_data <- crime_data_filtered[train_indices, ]
test_data <- crime_data_filtered[-train_indices, ]

# Train a Poisson regression model
poisson_model <- glm(OBJECTID ~ ., data = train_data, family = poisson)

# Make predictions on the test set
test_data$predictions <- predict(poisson_model, newdata = test_data, type = "response")

# Calculate the mean squared error of the model
mse <- mean((test_data$OBJECTID - test_data$predictions)^2)
print(paste("Mean Squared Error:", mse))

# Display the model's summary
summary(poisson_model)

```




```{r}
# Identify aliased coefficients
alias(poisson_model)

# Based on the output, remove or combine the problematic predictors
# For example, if REPORT_HOUR is causing the issue, remove it from the model

# Re-train the model without the aliased predictors
poisson_model_refined <- glm(OBJECTID ~ X + Y + REPORT_YEAR + REPORT_MONTH + REPORT_DAY, 
                             data = train_data, family = poisson)

# Check for multicollinearity again
library(car)
vif(poisson_model_refined)

# Make predictions on the test set
test_data$predictions <- predict(poisson_model_refined, newdata = test_data, type = "response")

# Calculate the mean squared error of the refined model
mse_refined <- mean((test_data$OBJECTID - test_data$predictions)^2)
print(paste("Refined Model Mean Squared Error:", mse_refined))

# Display the refined model's summary
summary(poisson_model_refined)

```
# ARIMA model

```{r}
class(crime_data2$REPORT_DAT)

library(forecast)
library(caret)
```

```{r}

monthly_data <- aggregate(REPORT_DAT ~ Month + Year + METHOD,
                          data = crime_data2[crime_data2$METHOD == "GUN", ], FUN = length)

weapon_ts <- ts(monthly_data$REPORT_DAT, start = c(2023, 1), frequency = 12)

arima_model <- auto.arima(weapon_ts)

ets_model <- ets(weapon_ts)


arima_forecast <- forecast(arima_model, h = 12)
ets_forecast <- forecast(ets_model, h = 12)

# Simple average of ARIMA and ETS forecasts (ensemble)
ensemble_forecast <- (arima_forecast$mean + ets_forecast$mean) / 2



# Plot the results
plot(arima_forecast, main = "Ensemble Forecast: ARIMA + ETS")
lines(ensemble_forecast, col = "red", lwd = 2, lty = 2)
legend("topright", legend = c("ARIMA", "ETS", "Ensemble"), 
       col = c("blue", "green", "red"), lty = 1:2, lwd = 2)



# Stack the forecasts for meta-modeling
stacked_data <- data.frame(arima = arima_forecast$mean, 
                           ets = ets_forecast$mean)

# Define the target variable (actual values for training purposes)
# Assume actual values for the next 12 months (need actual data here for proper training)
target <- monthly_data$REPORT_DAT[1:12]  # Replace with actual target data

# Train a meta-model (e.g., linear regression)
meta_model <- train(stacked_data, target, method = "lm")

# Use the trained meta-model to make final predictions
meta_predictions <- predict(meta_model, newdata = stacked_data)

# Plot the results
plot(arima_forecast, main = "Stacked Ensemble Forecast")
lines(meta_predictions, col = "red", lwd = 2, lty = 2)
legend("topright", legend = c("ARIMA", "ETS", "Stacked Ensemble"), 
       col = c("blue", "green", "red"), lty = 1:2, lwd = 2)


```

# arima model 2 


```{r}
library(forecast)
library(caret)



# Convert the REPORT_DAT column to Date format (if not already done)
crime_data2$REPORT_DAT <- as.Date(crime_data2$REPORT_DAT)

# Aggregate the data by month (count the number of "GUN" offenses per month)
monthly_data <- aggregate(REPORT_DAT ~ Month + Year + METHOD, data = crime_data2[crime_data2$METHOD == "GUN", ], FUN = length)

# Create a time series for "GUN" offenses per month
weapon_ts <- ts(monthly_data$REPORT_DAT, start = c(2023, 1), frequency = 12)

# Fit ARIMA model
arima_model <- auto.arima(weapon_ts)

# Fit Exponential Smoothing (ETS) model
ets_model <- ets(weapon_ts)

# Forecast using both models (forecast for the next 36 months - 3 years)
arima_forecast <- forecast(arima_model, h = 36)
ets_forecast <- forecast(ets_model, h = 36)

# Simple average of ARIMA and ETS forecasts (ensemble)
ensemble_forecast <- (arima_forecast$mean + ets_forecast$mean) / 2

# Plot the results
plot(arima_forecast, main = "Ensemble Forecast: ARIMA + ETS for 3 Years")
lines(ensemble_forecast, col = "red", lwd = 2, lty = 2)
legend("topright", legend = c("ARIMA", "ETS", "Ensemble"), 
       col = c("blue", "green", "red"), lty = 1:2, lwd = 2)







```


```{r}
sum(is.na(stacked_data))  # Show the count of missing values
head(stacked_data)

target_variance <- var(stacked_data$target)
if (target_variance == 0) {
  stop("Target variable has no variation; check your target data.")
}


meta_model <- train(stacked_data, target, method = "nnet", linout = TRUE, trace = FALSE)

meta_model <- train(stacked_data, target, method = "nnet", 
                    tuneGrid = expand.grid(size = c(5, 10, 15), decay = c(0, 0.1, 0.5)),
                    linout = TRUE, trace = FALSE)
print(meta_model)


str(crime_data2)
head(crime_data2)

head(crime_data2)

```


# new model for the crime patter per month 
```{r}




```





